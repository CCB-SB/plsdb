##################################################
# Config
##################################################
CONFIG_JSON = "pipeline.json"
configfile: CONFIG_JSON

##################################################
# Python modules
##################################################
import os
import re
import pandas
import pickle
import logging
import datetime
from tqdm import tqdm
from Bio import SeqIO
from collections import OrderedDict

from utils import setup_logger, run_cmd, mkdir

##################################################
# Some settings
##################################################

# Date
# today = datetime.datetime.now().strftime("%Y_%m_%d")
today = "2018_09_03"

# NCBI DBs
dbs = OrderedDict()
dbs['insd']   = 'INSDC'
dbs['refseq'] = 'RefSeq'

# conda path
tmp_c, tmp_s, CONDA_PATH = run_cmd('conda info --base')
assert tmp_s == 0
CONDA_PATH = CONDA_PATH.rstrip()

# conda env path
ENV_PATH = os.path.join(CONDA_PATH, 'envs', config['env_name'])

##################################################
# Tools
##################################################
# BLAST
BIN_MAKEBLASTDB = os.path.join(config["blast"]["dir"], "bin/makeblastdb")
BIN_BLASTN = os.path.join(config["blast"]["dir"], "bin/blastn")

# Mash
BIN_MASH = os.path.join(config["mash"]["dir"], "mash")

# edirect/eutils
EUTILS_INSTALL = os.path.join(config["eutils"]["dir"], "install.done")

# Krona
BIN_KRONA_XML = os.path.join(config["krona"]["dir"], "bin/ktImportXML")

# pmlst
PMLST_CLEAN = os.path.join(ENV_PATH, 'db/pubmlst/cleanup.done')
PMLST_ALLELES = os.path.join(ENV_PATH, 'db/pubmlst/download.done')
PMLST_DB = os.path.join(ENV_PATH, 'db/blast/mlst.fa')

##################################################
# Data
##################################################
# Table with plasmid records
PLASMIDS = [os.path.join(config['data']['odir']['reports'], '%s__%s__nuccore.txt' % (today, db)) for db in dbs.keys()]
PLASMIDS_ALL = os.path.join(config['data']['odir']['reports'], '%s__nuccore.txt' % today)
# linked assemblies
PLASMIDS_ALL_ASM = os.path.join(config['data']['odir']['reports'], '%s__nuccore_assembly.txt' % today)
# linked biosamples
PLASMIDS_ALL_BIOS = os.path.join(config['data']['odir']['reports'], '%s__nuccore_biosample.txt' % today)
# assembly info
PLASMIDS_ASM = os.path.join(config['data']['odir']['reports'], '%s__assembly.txt' % today)
# biosample info
PLASMIDS_BIOS = os.path.join(config['data']['odir']['reports'], '%s__biosample.txt' % today)
# taxonomy info (processed)
PLASMIDS_TAX = os.path.join(config['data']['odir']['reports'], '%s__taxa.txt' % today)
PLASMIDS_TAXP = os.path.join(config['data']['odir']['reports'], '%s__taxaproc.txt' % today)
# all info together
PLASMIDS_ALL_FULL = os.path.join(config['data']['odir']['reports'], '%s__full.txt' % today)

# filter1
PLASMIDS_FILT1        = os.path.join(config['data']['odir']['reports'], '%s__filtered1.txt' % today)
PLASMIDS_FILT1_FASTA  = os.path.join(config['data']['odir']['reports'], '%s__filtered1.fna' % today)
PLASMIDS_FILT1_SKETCH = os.path.join(config['data']['odir']['reports'], '%s__filtered1.msh' % today)
PLASMIDS_FILT1_DIST0  = os.path.join(config['data']['odir']['reports'], '%s__filtered1.dist0' % today)
PLASMIDS_FILT1_IDENT  = os.path.join(config['data']['odir']['reports'], '%s__filtered1.ident' % today)

# representative genomes
REPR_GENOMES = os.path.join(config['data']['odir']['chrom'], today, 'representative_genomes.tsv')
CHROM_FASTA  = os.path.join(config['data']['odir']['chrom'], today, 'chromosomes.fna')
CHROM_SKETCH  = os.path.join(config['data']['odir']['chrom'], today, 'chromosomes.msh')
PLASMIDS_CHROM_DIST = os.path.join(config['data']['odir']['chrom'], today, 'plasmids_chromosomes.dist')

# # rMLST
# PLASMIDS_FILT1_RMLST_FASTA     = os.path.join(config['data']['odir']['rmlst'], today, 'fastas.done')
# PLASMIDS_FILT1_RMLST_SUM       = os.path.join(config['data']['odir']['rmlst'], today, 'rmlst.tsv')

# filter 2
PLASMIDS_FILT2        = os.path.join(config['data']['odir']['reports'], '%s__filtered2.txt' % today)

# process locations
# PLASMIDS_LOC = os.path.join(config['data']['odir']['reports'], '%s__locations.txt' % today)

##################################################
# Data: "Master" files
##################################################
# # Plasmid sequences
# MASTER_GFASTA = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.fna" % today), db=dbs.keys())
#
# # ABRicate
# MASTER_ABRICATE_DBS = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.abr.{rdb}" % today), db=dbs.keys(), rdb=config['abricate']['dbs'])
# MASTER_ABRICATE = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.abr" % today), db=dbs.keys())
#
# # pMLST
# MASTER_PMLST = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.pmlst" % today), db=dbs.keys())
#
# # BLASTdb from master FASTA
# MASTER_BLASTDB = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.fna.{ext}" % today), db=dbs.keys(), ext=config['blast']['ext'])
#
# # Mash
# MASTER_MASH_SIG  = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.msh" % today), db=dbs.keys())
# MASTER_MASH_DIST = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.dist" % today), db=dbs.keys())
#
# # UMAP
# MASTER_MASH_UMAP = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.umap" % today), db=dbs.keys())
# MASTER_MASH_DIST = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.dist" % today), db=dbs.keys())
#
# # Master info table
# MASTER_TAB = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.tsv" % today), db=dbs.keys())
#
# # Krona plot
# MASTER_KRONA_XML = os.path.join(config["data"]["odir"]["master"], "%s.xml" % today)
# MASTER_KRONA_HTML = os.path.join(config["data"]["odir"]["master"], "%s.html" % today)
#
# # Other plots
# MASTER_PLOTS = os.path.join(config["data"]["odir"]["master"], "%s.pdf" % today)

##################################################
# Rules
##################################################

# ALL rule
##################################################
rule all:
	input:
        # tools
		blast=[BIN_MAKEBLASTDB, BIN_BLASTN],
        mash=BIN_MASH,
        eutils=EUTILS_INSTALL,
        krona=BIN_KRONA_XML,
        pmlstdb=[PMLST_CLEAN, PMLST_ALLELES, PMLST_DB],
        # plasmids
        query_plasmids=PLASMIDS_ALL_FULL,
        # plasmid filtering (1)
        filter_plasmids=PLASMIDS_FILT1,
        # identical plasmids
        identical_plasmids=PLASMIDS_FILT1_IDENT,
        # representative genomes
        chrom_dist=PLASMIDS_CHROM_DIST,
        # locations
        # locs=PLASMIDS_LOC,
		# # master files
        # # fasta
        # master_fasta=MASTER_GFASTA,
        # # abricate
        # abricate=MASTER_ABRICATE_DBS + [MASTER_ABRICATE],
        # # pmlst
        # pmlst=MASTER_PMLST,
        # # blastdb
		# master_blastdb=MASTER_BLASTDB,
		# # mash
        # master_sig=MASTER_MASH_SIG,
        # master_dist=MASTER_MASH_DIST,
        # # umap
        # master_umap=MASTER_MASH_UMAP,
        # # table
        # master_tab=MASTER_TAB,
        # # krona plot
        # master_krona=[MASTER_KRONA_XML, MASTER_KRONA_HTML],
        # # other plots
        # master_plots=MASTER_PLOTS,

# Tools
##################################################
# blast
rule install_blast:
    output:
        BIN_MAKEBLASTDB, BIN_BLASTN
    params:
        url=config["blast"]["url"],
        name=os.path.basename(config["blast"]["url"])
    message:
        "installing blast from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xzvf {params.name} && rm {params.name}"

# mash
rule install_mash:
    output:
        BIN_MASH
    params:
        url=config["mash"]["url"],
        name=os.path.basename(config["mash"]["url"])
    message:
        "installing mash from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xvf {params.name} && rm {params.name}"

# edirect/eutils
rule install_eutils:
    output:
        EUTILS_INSTALL
    message:
        "Install edirect/euils"
    shell:
        """
        mkdir -p $(dirname {output}) && cd tools
        perl -MNet::FTP -e \
            '$ftp = new Net::FTP("ftp.ncbi.nlm.nih.gov", Passive => 1);
             $ftp->login; $ftp->binary;
             $ftp->get("/entrez/entrezdirect/edirect.tar.gz");'
        gunzip -c edirect.tar.gz | tar xf -
        rm edirect.tar.gz
        cd edirect && wget https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/xtract.Linux.gz && gunzip -f xtract.Linux.gz && chmod +x xtract.Linux
        touch $(basename {output})
        """

# krona
rule install_krona:
    output:
        BIN_KRONA_XML
    message:
        "Install Krona"
    params:
        url=config["krona"]["url"],
        name=os.path.basename(config["krona"]["url"])
    shell:
        """
        mkdir -p tools && cd tools && wget {params.url} && unzip {params.name} && rm {params.name} &&
        rsync -av Krona-xl2.5/KronaTools . && rm -r Krona-xl2.5/ && cd KronaTools && ./install.pl --prefix .
        """

# pmlst
rule clean_pmlst:
    output:
        PMLST_CLEAN
    message:
        "Removing all schemes from mlst db"
    params:
        dir=ENV_PATH
    shell:
        """
        find {params.dir}/db/pubmlst/ -maxdepth 1 -mindepth 1 -type d -exec rm -r {{}} \; && \
        find {params.dir}/db/blast/ -type f -name 'mlst.*' -exec rm {{}} \; && touch {output}
        """

rule get_pmlst_data:
    input:
        PMLST_CLEAN
    output:
        PMLST_ALLELES
    message:
        "Download pMLST allele FASTA files"
    params:
        db=config['pmlst']['db'],
        url=config['pmlst']['url']
    run:
        # reference: https://github.com/kjolley/BIGSdb.git: scripts/rest_examples/python/download_alleles.py
        import requests
        from glob import glob
        from utils import proc_mlst_scheme_name
        setup_logger()

        url = params.url + '/db/' + params.db
        r = requests.get(url)
        if r.status_code == 404:
            print('Database ' + params.db + ' does not exist.')
            os._exit(1)

        # get loci
        for scheme_name, scheme_id in config['pmlst']['ids'].items():
            scheme_name2 = proc_mlst_scheme_name(scheme_name)
            print(scheme_name, scheme_name2, scheme_id)

            # where to save files
            scheme_dir = os.path.join(os.path.dirname(output[0]), scheme_name2)
            mkdir(scheme_dir, True)
            # where to save profiles
            scheme_profiles = os.path.join(scheme_dir, scheme_name2 + '.txt')

            # alleles
            r = requests.get(params.url +  '/db/' + params.db + '/schemes/' + str(scheme_id))
            assert r.status_code != 404, 'Scheme {} ({}) does not exist.'.format(scheme_name, scheme_id)
            loci = r.json()['loci']
            for locus_path in loci:
                r = requests.get(locus_path)
                locus = r.json()['id']
                if r.json()['alleles_fasta']:
                    r = requests.get(r.json()['alleles_fasta'])
                    fasta_file = open(scheme_dir + '/' + locus + '.tfa', 'w')
                    fasta_file.write(r.text)
                    fasta_file.close()

            # profiles
            r = requests.get(params.url +  '/db/' + params.db + '/schemes/' + str(scheme_id) + '/profiles_csv')
            if r.status_code == 404: # no profiles, create file with header and one dummy profile
                with open(scheme_profiles, 'w') as ofile:
                    locus = [ re.sub('\.tfa', '', os.path.basename(tfa)) for tfa in glob('%s/*.tfa' % scheme_dir) ]
                    ofile.write('ST\t{}\n{}\n'.format(
                        '\t'.join(locus),
                        '\t'.join(['1']*(len(locus)+1))
                    ))
                with open(scheme_profiles + '.dummy', 'w') as ofile:
                    ofile.write('dummy')
            else: # save profiles
                with open(scheme_profiles, 'w') as ofile:
                    ofile.write(re.sub('\t\n', '\n', r.text))
            # check formatting
            df = pandas.read_csv(scheme_profiles, sep='\t', header=0)
            if df.columns[0] != "ST":
                df.columns = ['ST'] + list(df.columns)[1:]
            if any([not re.fullmatch(r'\d+', str(st)) for st in list(df['ST'])]):
                df['oldST'] = df['ST'].copy()
                df['ST'] = list(range(1, df.shape[0]+1))
                df[[col for col in list(df.columns) if col != 'oldST']].to_csv(scheme_profiles, sep='\t', index=False, index_label=False)
                df.to_csv(scheme_profiles + '.old', sep='\t', index=False, index_label=False)

        with open(output[0], 'w') as ofile:
            ofile.write('done')

rule create_pmlst_db:
    input:
        PMLST_ALLELES
    output:
        PMLST_DB
    message:
        "Creating pMLST db"
    params:
        dir=ENV_PATH
    shell:
        "{params.dir}/scripts/mlst-make_blast_db"

# Plasmid query
##################################################
rule query_plasmids:
    output:
        "{dir}/{date}__{db}__nuccore.txt"
    message:
        "Query for plasmids in {wildcards.db}"
    params:
        query=lambda wildcards: config['eutils']['query']['plasmid']['cmd'].format(
            path=config['eutils']['dir'],
            esearch_query=config['eutils']['query']['plasmid']['esearch_query'],
            efilter_query=config['eutils']['query']['plasmid']['efilter_query'].format(db=wildcards.db),
            xtract_query=config['eutils']['query']['plasmid']['xtract_query']
        ),
        header='\t'.join(config['eutils']['header']['plasmid'])
    shell:
        """
        mkdir -p $(dirname {output}) && echo -e \"{params.header}\" > {output} && {params.query} >> {output}
        """

rule queried_plasmids:
    input:
        PLASMIDS
    output:
        PLASMIDS_ALL
    message:
        "Creating a table of all found plasmids"
    run:
        dfs = []
        for df_file in input:
            df = pandas.read_csv(df_file, sep='\t', header=0)
            df_source = df_file.split('__')[1]
            assert df_source in dbs.keys(), 'Unknown source \"{}\"'.format(df_source)
            df['Source_NUCCORE'] = df_source
            dfs.append(df)
        dfs = pandas.concat(dfs)
        dfs.to_csv(output[0], sep='\t', index=False, header=True)

# Linked assemblies
rule query_linked_assemblies:
    input:
        PLASMIDS_ALL
    output:
        PLASMIDS_ALL_ASM
    message:
        "Query for linked Assemblies from {input}"
    run:
        from utils import run_epost_split
        setup_logger(logging.INFO)
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['link_asm']),
            cmd=config['eutils']['query']['linked'],
            df_col='UID_NUCCORE',
            split_size=300,
            split_str=None,
            path=config['eutils']['dir'],
            db_source='nuccore',
            db_target='assembly'
        )

rule query_assemblies:
    input:
        PLASMIDS_ALL_ASM
    output:
        PLASMIDS_ASM
    message:
        "Query for Assemblies from {input}"
    run:
        from utils import run_epost_split
        setup_logger(logging.INFO)
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['assembly']),
            cmd=config['eutils']['query']['assembly'],
            df_col='UID_ASSEMBLY',
            split_size=300,
            split_str=';',
            path=config['eutils']['dir']
        )

# Linked biosamples
rule query_linked_biosamples:
    input:
        PLASMIDS_ALL
    output:
        PLASMIDS_ALL_BIOS
    message:
        "Query for linked BioSamples from {input}"
    run:
        from utils import run_epost_split
        setup_logger(logging.INFO)
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['link_bios']),
            cmd=config['eutils']['query']['linked'],
            df_col='UID_NUCCORE',
            split_size=300,
            split_str=None,
            path=config['eutils']['dir'],
            db_source='nuccore',
            db_target='biosample'
        )

rule query_biosamples:
    input:
        PLASMIDS_ALL_BIOS
    output:
        PLASMIDS_BIOS
    message:
        "Query for BioSamples from {input}"
    run:
        from utils import run_epost_split
        setup_logger(logging.INFO)
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['biosample']),
            cmd=config['eutils']['query']['biosample'],
            df_col='UID_BIOSAMPLE',
            split_size=300,
            split_str=';',
            path=config['eutils']['dir']
        )

# Linked taxonomy
rule query_taxon:
    input:
        PLASMIDS_ALL
    output:
        PLASMIDS_TAX
    message:
        "Query for linked Taxa from {input}"
    run:
        setup_logger(logging.INFO)

        ids = pandas.read_csv(input[0], sep='\t', header=0, dtype=str)['TaxonID_NUCCORE'].fillna(value='')
        ids = set(ids.values)
        assert '' not in ids, 'Have an empty ID in {}'.format(input[0])
        logging.info('There are {} unique taxonomy IDs'.format(len(ids)))

        cmd = "{query} > {ofile}".format(
            query=config['eutils']['query']['taxon'].format(
                path=config['eutils']['dir'],
                ids=','.join(ids)
            ),
            ofile=output[0]
        )
        cmd, cmd_s, cmd_o = run_cmd(cmd)
        assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)

rule proc_taxon:
    input:
        PLASMIDS_TAX
    output:
        PLASMIDS_TAXP
    message:
        "Process taxa from {input}"
    run:
        header = config['eutils']['header']['taxon']
        ranks  = config['eutils']['header']['ranks']
        for rank in ranks:
            header += ['taxon_%s_id' % rank, 'taxon_%s_name' % rank]

        with open(output[0], 'w') as ofile:
            ofile.write('\t'.join(header) + '\n')
            with open(input[0], 'r') as ifile:
                for line in ifile:
                    info = dict.fromkeys(header,'NA')
                    line = line.rstrip('\n')
                    for field in line.split('\t'):
                        if re.search('\|',field):
                            tid, tname, trank = field.split('|')
                            if trank == 'no rank':
                                trank = 'NA'
                            elif "taxon_%s_name" % trank in header:
                                info["taxon_%s_name" % trank] = tname
                                info["taxon_%s_id" % trank] = tid
                            if info['taxon_id'] == 'NA':
                                info['taxon_id'] = tid
                                info['taxon_name'] = tname
                                info['taxon_rank'] = trank
                        else: # lineage
                            info['lineage'] = field
                    assert info['taxon_id'] != "NA" and info['taxon_name'] != "NA"
                    ofile.write('\t'.join([info[k] for k in header]) + '\n')

# Add to table
rule collect_meta:
    input:
        pls=PLASMIDS_ALL,
        asm=PLASMIDS_ASM,
        bios=PLASMIDS_BIOS,
        tax=PLASMIDS_TAXP,
        pls_asm=PLASMIDS_ALL_ASM,
        pls_bios=PLASMIDS_ALL_BIOS,
    output:
        PLASMIDS_ALL_FULL
    message:
        "Adding meta info to {input.pls}"
    run:
        import datetime
        setup_logger(logging.INFO)

        pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
        pls.set_index('UID_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logging.info('Read in {} plasmid records\n{}'.format(pls.shape[0], pls.head()))

        asm = pandas.read_csv(input.asm, sep='\t', header=0, dtype=str)
        asm.set_index('UID_ASSEMBLY', drop=True, inplace=True, verify_integrity=True)
        logging.info('Read in {} assembly records\n{}'.format(asm.shape[0], asm.head()))

        tax = pandas.read_csv(input.tax, sep='\t', header=0, dtype=str)
        tax.set_index('taxon_id', drop=True, inplace=True, verify_integrity=True)
        logging.info('Read in {} taxonomy records\n{}'.format(tax.shape[0], tax.head()))

        bios = pandas.read_csv(input.bios, sep='\t', header=0, dtype=str)
        bios.set_index('UID_BIOSAMPLE', drop=True, inplace=True, verify_integrity=True)
        logging.info('Read in {} biosample records\n{}'.format(bios.shape[0], bios.head()))

        pls_asm = pandas.read_csv(input.pls_asm, sep='\t', header=0, dtype=str)
        pls_asm.set_index('UID_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        logging.info('Read in {} plasmid/assembly records\n{}'.format(pls_asm.shape[0], pls_asm.head()))

        pls_bios = pandas.read_csv(input.pls_bios, sep='\t', header=0, dtype=str)
        pls_bios.set_index('UID_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        logging.info('Read in {} plasmid/assembly records\n{}'.format(pls_bios.shape[0], pls_bios.head()))

        # process plasmid/assembly mapping - only latest or none
        def get_latest_assembly(uids):
            if pandas.isnull(uids):
                return None
            uids = uids.split(';')
            if len(uids) == 1:
                return uids[0]
            for uid in uids:
                if asm.loc[uid,'Latest_ASSEMBLY'] == 'True':
                    return uid
            # get dates, parse, sort, get first
            dates = asm.loc[uids,'SeqReleaseDate_ASSEMBLY'].map(lambda x: datetime.datetime.strptime(x, '%Y/%m/%d %H:%M'))
            dates.sort_values(ascending=False, inplace=True)
            logging.warn('No latest assembly among {}: latest w.r.t. date is {}'.format(uids, dates.index[0]))
            return dates.index[0]

        pls_asm['UID_ASSEMBLY'] = pls_asm['UID_ASSEMBLY'].map(get_latest_assembly)

        # add assembly ID
        pls = pandas.merge(
            left=pls,
            right=pls_asm,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        # add assembly info
        pls = pandas.merge(
            left=pls,
            right=asm,
            how='left',
            left_on='UID_ASSEMBLY',
            right_index=True,
            sort=False,
        )

        # add biosample ID
        pls = pandas.merge(
            left=pls,
            right=pls_bios,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        # add biosample info
        pls = pandas.merge(
            left=pls,
            right=bios,
            how='left',
            left_on='UID_BIOSAMPLE',
            right_index=True,
            sort=False,
        )

        # add taxonomy
        pls = pandas.merge(
            left=pls,
            right=tax,
            how='left',
            left_on='TaxonID_NUCCORE',
            right_index=True,
            sort=False,
        )

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Plasmid filtering (1)
##################################################
rule filter1:
    input:
        PLASMIDS_ALL_FULL
    output:
        PLASMIDS_FILT1
    message:
        "Filter plasmids from {input}"
    params:
        dfilter=config['filtering']['dfilter']
    run:
        setup_logger(logging.INFO)
        pls = pandas.read_csv(input[0], sep='\t', header=0, dtype=str)
        logging.info('Read in {} plasmid records'.format(pls.shape[0]))

        # filter by genome tag
        pls = pls.loc[pls['Genome_NUCCORE'] == 'plasmid',:]
        logging.info('Filtered: genome tag "plasmid": kept {} records'.format(pls.shape[0]))

        # filter by description
        keep_rows = pandas.Series([re.search(params.dfilter, d, re.IGNORECASE) is None for d in pls['Description_NUCCORE']], index=pls.index)
        pls = pls.loc[keep_rows,:]
        logging.info('Filtered: description "{}": kept {} records'.format(params.dfilter, pls.shape[0]))

        # filter by (assembly) completeness
        def pls_filter(uid):
            if pandas.isnull(pls.loc[uid, 'UID_ASSEMBLY']):
                return pls.loc[uid, 'Completeness_NUCCORE'] == 'complete'
            else:
                return pls.loc[uid, 'Status_ASSEMBLY'] == 'Complete Genome'

        pls = pls.loc[pls.index.map(pls_filter),:]
        logging.info('Filtered: (assembly) completeness: kept {} records'.format(pls.shape[0]))

        # filter by taxonomy
        pls = pls.loc[pls['taxon_superkingdom_id'] == '2',:]
        logging.info('Filtered: superkingdom ID is 2 (i.e. Bacteria): kept {} records'.format(pls.shape[0]))

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Identical plasmids
##################################################
# FASTA
rule get_fasta:
    input:
        "{basename}.txt"
    output:
        "{basename}.fna"
    message:
        "Downloading sequences for IDs in {input}"
    params:
        eutils=config['eutils']['dir']
    shell:
        """
        mkdir -p $(dirname {output}) &&
        python download_fastas.py -t {input} -i \"UID_NUCCORE\" -o {output} -c 10 -s 100 -e {params.eutils} 2>&1 | tee {output}.log &&
        cat {output}.tmp.* > {output} && rm {output}.tmp.*
        """

# Mash sketch
rule mash_sketch:
    input:
        "{basename}.fna"
    output:
        "{basename}.msh"
    params:
        params=config['mash']['sketch_params'],
        mash=BIN_MASH
    message:
        "Create Mash signatures from {input}"
    shell:
        "{params.mash} sketch {params.params} -o $(dirname {output})/$(basename -s .msh {output}) {input}"

# Mash dist (only if 0)
rule mash_dist_zero:
    input:
        "{basename}.msh"
    output:
        "{basename}.dist0"
    params:
        params=config['mash']['dist0_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input} {input} > {output}"

# Assign I-group IDs (and some other stuff)
rule identical_plasmids:
    input:
        pls=PLASMIDS_FILT1,
        fna=PLASMIDS_FILT1_FASTA,
        dist=PLASMIDS_FILT1_DIST0
    output:
        PLASMIDS_FILT1_IDENT
    message:
        "Group identical plasmids from {input.pls}"
    run:
        from Bio import SeqIO
        from Bio.SeqUtils import GC
        setup_logger(logging.INFO)

        pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
        pls.set_index('ACC_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logging.info('Read in {} plasmid records'.format(pls.shape[0]))

        # collect seq. stats.s
        seq = []
        seq_seq = {}
        with open(input.fna, 'r') as ifile:
            for record in tqdm(SeqIO.parse(ifile, 'fasta')):
                assert re.fullmatch(r'\w+\.\d+', record.id), 'Unexpected ID format in FASTA: {}'.format(record.id)
                rid = record.id.split('.')[0]
                seq.append({
                    'ACC_FASTA': record.id,
                    'ACC_NUCCORE': rid,
                    'GC_NUCCORE': GC(record.seq),
                    'Length': len(record.seq)
                })
                seq_seq[record.id] = record.seq
        seq = pandas.DataFrame(seq)
        seq.set_index('ACC_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        # add to table
        pls = pandas.merge(
            left=pls,
            right=seq,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        # check seq. length
        assert pls['Length_NUCCORE'].astype(int).equals(pls['Length']), 'Sequence length is not always identical: nuccore vs. FASTA'
        # drop 2nd seq. len. col.
        pls.drop(columns=['Length'], inplace=True)
        # change index
        pls.set_index('ACC_FASTA', drop=False, inplace=True, verify_integrity=True)

        # find and assign identity groups
        pls['UID_IDENTGROUP'] = None
        groupID = 0
        with open(input.dist, 'r') as ifile:
            for line in tqdm(ifile):
                sID, qID, dist, pv, sh = line.rstrip('\n').split('\t')
                # same ID -> skip
                if sID == qID:
                    continue
                # group ID already set -> skip
                if pandas.notnull(pls.loc[sID, 'UID_IDENTGROUP']) and pandas.notnull(pls.loc[qID, 'UID_IDENTGROUP']):
                    continue
                # check if really equal
                if len(seq_seq[sID]) != len(seq_seq[qID]):
                    continue
                elif seq_seq[sID] != seq_seq[qID]: # length is equal -> check sequences
                    continue
                # sequences are identical
                # one of the seq.s has already an ID -> set to same value
                if pandas.notnull(pls.loc[sID, 'UID_IDENTGROUP']):
                    pls.loc[qID, 'UID_IDENTGROUP'] = pls.loc[sID, 'UID_IDENTGROUP']
                    continue
                if pandas.notnull(pls.loc[qID, 'UID_IDENTGROUP']):
                    pls.loc[sID, 'UID_IDENTGROUP'] = pls.loc[qID, 'UID_IDENTGROUP']
                    continue
                # new group -> set both to new value
                groupID += 1
                pls.loc[sID, 'UID_IDENTGROUP'] = groupID
                pls.loc[qID, 'UID_IDENTGROUP'] = groupID

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Chromosomes
##################################################
rule get_representative_genomes:
    output:
        REPR_GENOMES
    params:
        url = config['representative_genomes']
    message:
        "Download table of representative genomes from {params.url}"
    run:
        cmd = "mkdir -p {} && wget {} -O {}".format(os.path.dirname(output[0]), params.url, output[0])
        cmd, cmd_s, cmd_o = run_cmd(cmd)
        assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)

        df = pandas.read_csv(output[0], sep='\t', header=0)
        ids = []
        for s in df['Chromosome RefSeq']:
            if pandas.isnull(s) or s == "":
                continue
            ids += s.split(',')
        ids = pandas.Series(ids).to_frame('RefSeqAcc')
        ids.to_csv(output[0], sep='\t', header=True, index=False)

rule get_chromosome_fasta:
    input:
        REPR_GENOMES
    output:
        CHROM_FASTA
    message:
        "Download FASTAs from {input}"
    params:
        scr="download_fastas.py",
        col="RefSeqAcc",
        cores=10,
        eutils=config['eutils']['dir']
    shell:
        """
        mkdir -p $(dirname {output}) &&
        python {params.scr} -t {input} -i {params.col} -o {output} -s 50 -c {params.cores} -f acc -e {params.eutils} 2>&1 | tee {output}.log &&
        cat {output}.tmp.* > {output} && rm {output}.tmp.*
        """

rule pls_chrom_dist:
    input:
        msh_pls=PLASMIDS_FILT1_SKETCH,
        msh_chrom=CHROM_SKETCH
    output:
        PLASMIDS_CHROM_DIST
    params:
        params=config['mash']['dist_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input.msh_chrom} {input.msh_pls} > {output}"

# Plasmid filtering (2)
##################################################
# # FASTA(s) for checkM
# rule fastas_for_rmlst:
#     input:
#         pls=PLASMIDS_FILT1_IDENT,
#         fna=PLASMIDS_FILT1_FASTA
#     output:
#         PLASMIDS_FILT1_RMLST_FASTA
#     message:
#         'Creating FASTA files for checkM from {input}'
#     run:
#         from utils import mkdir
#         from Bio import SeqIO
#
#         setup_logger(logging.INFO)
#         mkdir(os.path.dirname(output[0]), True)
#
#         # plasmids
#         pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
#         pls.set_index('ACC_FASTA', drop=False, inplace=True, verify_integrity=True)
#         logging.info('Read in {} plasmid records'.format(pls.shape[0]))
#
#         # select unique plasmids
#         groups = set()
#         sel = set()
#         for sid in pls.index:
#             if pandas.isnull(pls.loc[sid, 'UID_IDENTGROUP']):
#                 sel.add(sid)
#             elif pls.loc[sid, 'UID_IDENTGROUP'] not in groups:
#                 sel.add(sid)
#                 groups.add(pls.loc[sid, 'UID_IDENTGROUP'])
#         logging.info('Selected {} unique plasmids'.format(len(sel)))
#
#         # collect seq. stats.s
#         with open(input.fna, 'r') as ifile:
#             for record in tqdm(SeqIO.parse(ifile, 'fasta')):
#                 if record.id in sel:
#                     SeqIO.write(record, os.path.join(os.path.dirname(output[0]), '%s.fna' % record.id), 'fasta')
#         with open(output[0], 'w') as ofile:
#             ofile.write('Created %d FASTA files' % len(sel))
#
# rule rmlst:
#     input:
#         PLASMIDS_FILT1_RMLST_FASTA
#     output:
#         PLASMIDS_FILT1_RMLST_SUM
#     params:
#         cores=5
#     run:
#         # Reference: https://pubmlst.org/rmlst/api.shtml
#         from multiprocessing import Pool
#         from glob import glob
#         from utils import run_rmlst
#
#         pool = Pool(params.cores)
#         df = pool.map(run_rmlst, glob('%s/*.fna' % os.path.dirname(input[0])))
#         pool.close()
#         pool.join()
#
#         # df = []
#         # for fasta in tqdm(glob('%s/*.fna' % os.path.dirname(input[0]))):
#         #     df.append(run_rmlst(fasta))
#
#         # convert
#         df = pandas.DataFrame(df)
#         # save
#         df.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)
#
# rule filter2:
#     input:
#         pls=PLASMIDS_FILT1_IDENT,
#         rmlst=PLASMIDS_FILT1_RMLST_SUM
#     output:
#         PLASMIDS_FILT2
#     message:
#         "Filter based on rMLST results"
#     run:
#         setup_logger(logging.INFO)
#
#         pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
#         pls.set_index('ACC_FASTA', drop=False, inplace=True, verify_integrity=True)
#         logging.info('Read in {} plasmid records\n{}'.format(pls.shape[0], pls.head()))
#
#         rmlst = pandas.read_csv(input.rmlst, sep='\t', header=0, dtype=str)
#         rmlst.set_index('ACC_FASTA', drop=True, inplace=True, verify_integrity=True)
#         logging.info('Read in {} rMLST records\n{}'.format(rmlst.shape[0], rmlst.head()))
#
#         # merge together
#         pls = pandas.merge(
#             left=pls,
#             right=rmlst,
#             how='left',
#             left_index=True,
#             right_index=True,
#             sort=False,
#         )
#         # add values for ident. sequences
#         for sid in pls.index:
#             if pandas.isnull(pls.loc[sid, 'Support_RMLST']):
#                 continue
#             if pandas.isnull(pls.loc[sid, 'UID_IDENTGROUP']):
#                 continue
#             sid_ident = pls['UID_IDENTGROUP'] == pls.loc[sid, 'UID_IDENTGROUP']
#             for col in rmlst.columns:
#                 pls.loc[sid_ident, col] =  pls.loc[sid, col]
#
#         # filter
#         pls = pls.loc[pls['Support_RMLST'].astype(float) >= 0,:]
#         logging.info('Filtered: rMLST support": kept {} records'.format(pls.shape[0]))
#
#         # drop rMLST columns
#         pls.drop(columns=rmlst.columns, inplace=True)
#
#         # save
#         pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Locations
##################################################
# rule parse_locations:
#     input:
#         PLASMIDS_ALL_FULL
#     output:
#         PLASMIDS_LOC
#     message:
#         "Parsing locations from {input}"
#     params:
#         api_keys=config['data']['api_keys'],
#         locs=config['data']['locs']
#     run:
#         from utils import preproc_loc_str, preproc_loc_coords, parse_location
#         from utils import load_locs, save_locs, update_locs
#         setup_logger(logging.INFO)
#
#         # api key
#         with open(params.api_keys, 'r') as ifile:
#             api_key = [k.rstrip('\n').strip() for k in ifile.readlines()]
#             api_key = api_key.pop()
#         logging.info('Using API key: {}'.format(api_key))
#
#         # load known locations
#         locs = load_locs(params.locs)
#
#         # sample table with locations
#         pls = pandas.read_csv(input[0], header=0, sep='\t')
#         pls['loc_lat'] = None
#         pls['loc_lng'] = None
#
#         # parse
#         for i in pls.index:
#             # location name and coordinates
#             l_n = pls.loc[i,'Location_BIOSAMPLE']
#             l_c = pls.loc[i,'Coordinates_BIOSAMPLE']
#
#             # pre-processing
#             l_n = preproc_loc_str(l_n)
#             l_c = preproc_loc_coords(preproc_loc_str(l_c))
#
#             # no location data
#             if l_n is None and l_c is None:
#                 continue
#
#             # at least name or coordinates given
#             if l_c is not None:
#                 l_c_str = '{};{}'.format(l_c[0], l_c[1])
#                 if locs is None or l_c_str not in locs['location']:
#                     logging.info('Retrieving coordinates for location: \"{}\"'.format(l_c))
#                     try:
#                         parsed = parse_location(loc_str=l_c, api_key=api_key, is_name=False)
#                         locs = update_locs(locs, {'location': l_c_str, 'type': 'coordinates', 'lat': parsed['lat'], 'lng': parsed['lng']})
#                         save_locs(locs, params.locs)
#                     except Exception as e:
#                         logging.error('Error while retrieving coordinates for location \"{}\"'.format(l_c))
#                         raise(e)
#                 pls.loc[i,'loc_lat'] = locs.loc[l_c_str,'lat']
#                 pls.loc[i,'loc_lng'] = locs.loc[l_c_str,'lng']
#             elif l_n is not None:
#                 if locs is None or l_n not in locs['location']:
#                     logging.info('Retrieving coordinates for location: \"{}\"'.format(l_n))
#                     try:
#                         parsed = parse_location(loc_str=l_n, api_key=api_key, is_name=True)
#                         locs = update_locs(locs, {'location': l_n, 'type': 'name', 'lat': parsed['lat'], 'lng': parsed['lng']})
#                         save_locs(locs, params.locs)
#                     except Exception as e:
#                         logging.error('Error while retrieving coordinates for location \"{}\"'.format(l_n))
#                         raise(e)
#                 pls.loc[i,'loc_lat'] = locs.loc[l_n,'lat']
#                 pls.loc[i,'loc_lng'] = locs.loc[l_n,'lng']
#
#         # save
#         pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# # FASTAs
# ##################################################
# # Download FASTAs
# rule download_seqs:
#     input:
#         "%s/{date}__{db}__filtered.txt" % config['data']['odir']['reports']
#     output:
#         "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
#     message:
#         "Downloading sequences for IDs in {input}"
#     params:
#         eutils=config['eutils']['dir']
#     shell:
#         """
#         mkdir -p $(dirname {output}) &&
#         python download_fastas.py -t {input} -i \"UID_NUCCORE\" -o {output} -c 10 -s 100 -e {params.eutils} 2>&1 | tee {output}.log &&
#         cat {output}.tmp.* > {output} && rm {output}.tmp.*
#         """
#
# # ABRicate annot
# ##################################################
# # NOTE: To have more control over the BLAST search and filtering ABRicate is not used directly.
# #       BLASTn is run using ABRicate DBs and the results are filtered by indentity and coverage
# #       analogous to the ABRicate implementation.
# rule abricate:
#     input:
#         "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
#     output:
#         "%s/{date}__{db}.abr.{rdb}" % config["data"]["odir"]["master"]
#     message:
#         "ABRicate on fasta: {input}"
#     params:
#         blastn=BIN_BLASTN,
#         cores=10
#     run:
#         setup_logger()
#         header = config['abricate']['blastn']['header']
#         cov = config['abricate']['params'][wildcards.rdb]['cov']
#         ident = config['abricate']['params'][wildcards.rdb]['ident']
#
#         # blastn search
#         cmd = config['abricate']['blastn']['cmd'].format(
#             blastn=params.blastn,
#             input=input[0],
#             rdb=wildcards.rdb,
#             ident=ident,
#             output=output[0],
#             header=' '.join(header),
#             cores=params.cores
#         )
#         logging.info(cmd)
#         cmd, cmd_s, cmd_o = run_cmd(cmd)
#         assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)
#
#         # read in results
#         df = pandas.read_csv(output[0], sep='\t', header=None, names=header)
#
#         # compute coverage
#         # https://github.com/tseemann/abricate/blob/f38e303611291336a7dbb01e94c22fd894e51016/bin/abricate#L135
#         df['cov'] =  100 * (df['length'] - df['gaps']) / df['slen']
#         # filter by coverage
#         df = df.loc[df['cov'] >= cov,:]
#
#         # sseqid -> DB + ID
#         df['sseqdb'] = df['sseqid'].map(lambda x: x.split('~~~')[0])
#         df['sseqid'] = df['sseqid'].map(lambda x: ', '.join(x.split('~~~')[1:]))
#
#         # save
#         df.to_csv(
#             path_or_buf=output[0],
#             sep='\t',
#             header=True,
#             index=False,
#             index_label=False
#         )
#
# rule cat_abricate:
#     input:
#         ["%s/{date}__{db}.abr.%s" % (config["data"]["odir"]["master"], db) for db in config['abricate']['dbs']]
#     output:
#         "%s/{date}__{db}.abr" % config["data"]["odir"]["master"]
#     message:
#         "Concat ABRicate hits: {input}"
#     params:
#         blastn=BIN_BLASTN,
#         cores=10
#     run:
#         dfs = []
#         for ifile in input:
#             dfs.append(
#                 pandas.read_csv(ifile, sep='\t', header=0)
#             )
#         dfs = pandas.concat(dfs, axis=0)
#         dfs.to_csv(
#             path_or_buf=output[0],
#             sep='\t',
#             header=True,
#             index=False,
#             index_label=False
#         )
#
# # pMLST
# #################################################
# rule pmlst:
#     input:
#         db=PMLST_DB,
#         fasta="%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
#     output:
#         "%s/{date}__{db}.pmlst" % config["data"]["odir"]["master"]
#     message:
#         "pmlst on fasta: {input}"
#     params:
#         cmd=config['pmlst']['cmd']
#     run:
#         import shutil
#         from Bio import SeqIO
#         from utils import reproc_mlst_scheme_name
#         setup_logger()
#
#         # split multi FASTA to sep. files
#         tmp_dir = 'tmp_fasta_%s' % wildcards.db
#         tmp_file = os.path.join(tmp_dir, 'pmlst.tsv')
#         assert not os.path.exists(tmp_dir)
#         assert not os.path.exists(tmp_file)
#         os.makedirs(tmp_dir)
#         logging.info('Splitting %s into %s/' % (input.fasta, tmp_dir))
#         with open(input.fasta, 'r') as ifile:
#             for record in SeqIO.parse(ifile, 'fasta'):
#                 SeqIO.write(record, open(os.path.join(tmp_dir, record.id + '.fasta'), 'w'), 'fasta')
#
#         # run pmlst
#         cmd = params.cmd + " {idir}/*.fasta > {ofile}".format(idir=tmp_dir, ofile=tmp_file)
#         logging.info('Running pmlst: %s' % cmd)
#         cmd, cmd_s, cmd_o = run_cmd(cmd)
#         assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)
#
#         # profiles
#         profiles = {}
#
#         # process
#         logging.info('Processing')
#         with open(output[0], 'w') as ofile, open(tmp_file, 'r') as ifile:
#             for line in ifile:
#                 line = line.rstrip('\n').split('\t')
#                 # no hit
#                 if line[1] == "-":
#                     continue
#                 # process hit
#                 line[0] = re.sub('\.fasta', '', os.path.basename(line[0])) # ID
#                 if line[2] != "-":
#                     pfile = os.path.join(ENV_PATH, 'db/pubmlst', line[1], line[1] + '.txt')
#                     if line[1] not in profiles:
#                         if os.path.exists(pfile + '.dummy'):
#                             profiles[line[1]] = 'dummy'
#                         elif os.path.exists(pfile + '.old'):
#                             profiles[line[1]] = pandas.read_csv(pfile + '.old', sep='\t', header=0)
#                         else:
#                             profiles[line[1]] = None
#                     if profiles[line[1]] is None:
#                         continue
#                     elif isinstance(profiles[line[1]], str) and profiles[line[1]] == 'dummy': # dummy STs
#                         logging.info('Dummy ST: %s' % line[0])
#                         line[2] = '-'
#                     elif 'oldST' in list(profiles[line[1]]): # ST mapping
#                         logging.info('Mapped ST: %s' % line[0])
#                         line[2] = (profiles[line[1]].loc[profiles[line[1]]['ST'] == int(line[2]),'oldST'].tolist())[0]
#                 line[1] = reproc_mlst_scheme_name(line[1])
#                 # save
#                 ofile.write("{}\t{}({}): {}\n".format(
#                     line[0],
#                     line[1],
#                     line[2],
#                     ';'.join(line[3:])
#                 ))
#
#         # rm tmp dir
#         shutil.rmtree(tmp_dir)
#
# # BLASTn DBs
# ##################################################
# rule blastndb:
#     input:
#         "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
#     output:
#         ["%s/{date}__{db}.fna.%s" % (config["data"]["odir"]["master"], ext) for ext in config['blast']['ext']]
#     log:
#         "%s/{date}__{db}.makeblastdb.log" % config["data"]["odir"]["master"]
#     params:
#         bin=BIN_MAKEBLASTDB
#     message:
#         "Create BLASTDB from {input}"
#     shell:
#         "{params.bin} -in {input} -input_type fasta -dbtype nucl -title \"ncbi__{wildcards.db}__{wildcards.date}\" -logfile {log}"
#
# # Mash
# ##################################################
# # signatures
# rule mash_sig:
#     input:
#         "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
#     output:
#         "%s/{date}__{db}.msh" % config["data"]["odir"]["master"]
#     log:
#         "%s/{date}__{db}.msh.log" % config["data"]["odir"]["master"]
#     params:
#         params=config['mash']['sketch_params'],
#         mash=BIN_MASH
#     message:
#         "Create signatures using Mash on {input}"
#     shell:
#         "{params.mash} sketch {params.params} -o $(dirname {output})/$(basename -s .msh {output}) {input} 2>&1 | tee {log}"
#
# # distance matrix
# rule mash_dist:
#     input:
#         "%s/{date}__{db}.msh" % config["data"]["odir"]["master"]
#     output:
#         "%s/{date}__{db}.dist" % config["data"]["odir"]["master"]
#     log:
#         "%s/{date}__{db}.dist.log" % config["data"]["odir"]["master"]
#     params:
#         params=config['mash']['dist_params'],
#         mash=BIN_MASH
#     message:
#         "Compare signatures in {input}"
#     shell:
#         "{params.mash} dist {params.params} {input} {input} > {output} 2> {log}"
#
# # Embedding
# ##################################################
# rule umap:
#     input:
#         "%s/{date}__{db}.dist" % config["data"]["odir"]["master"]
#     output:
#         "%s/{date}__{db}.umap" % config["data"]["odir"]["master"]
#     log:
#         "%s/{date}__{db}.umap.log" % config["data"]["odir"]["master"]
#     params:
#         neighbors=config['umap']['neighbors'],
#         components=config['umap']['components']
#     message:
#         "UMAP on {input}"
#     run:
#         import umap
#         import numpy
#         setup_logger(logging.INFO)
#         # IDs
#         ids = None
#         with open(input[0], 'r') as ifile:
#             ids = ifile.readline().rstrip('\n').split('\t')[1:]
#             logging.info('Distance matrix: {num} x {num}'.format(num=len(ids)))
#         # distance matrix
#         logging.info('Start loading distances...')
#         dist = numpy.loadtxt(
#             fname=input[0],
#             skiprows=1,
#             usecols=range(1,len(ids) + 1) # skip 1st (contains IDs)
#         )
#         logging.info('Done.')
#         # embedding
#         logging.info('Start embedding...')
#         embedding = umap.UMAP(
#             n_neighbors=params.neighbors,
#             n_components=params.components,
#             init='random',
#             metric='precomputed'
#         ).fit_transform(dist)
#         logging.info('Done.')
#         # save to file
#         embedding = pandas.DataFrame(
#             embedding,
#             columns=['D1', 'D2'],
#             index=ids
#         )
#         embedding.to_csv(
#             path_or_buf=output[0],
#             sep='\t',
#             header=True,
#             index=True,
#             index_label='ID'
#         )
#
# # Info table
# ##################################################
# rule infotable:
#     input:
#         plasmids="%s/{date}__{db}__filtered.txt" % config["data"]["odir"]["reports"],
#         linked_samples="%s/{date}__{db}__linkbiosamples.txt" % config["data"]["odir"]["reports"],
#         samples="%s/{date}__{db}__biosamples_locs.txt" % config["data"]["odir"]["reports"],
#         taxa="%s/{date}__{db}__taxaproc.txt" % config["data"]["odir"]["reports"],
#         embedding="%s/{date}__{db}.umap" % config["data"]["odir"]["master"],
#         plasmidfinder="%s/{date}__{db}.abr.plasmidfinder" % config["data"]["odir"]["master"],
#         pmlst="%s/{date}__{db}.pmlst" % config["data"]["odir"]["master"],
#         fasta="%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
#     output:
#         "%s/{date}__{db}.tsv" % config["data"]["odir"]["master"]
#     message:
#         "Create info table for records in {input.plasmids}"
#     run:
#         from Bio import SeqIO
#         from Bio.SeqUtils import GC
#         setup_logger(logging.INFO)
#
#         # read in data
#         # plasmids
#         pls = pandas.read_csv(input.plasmids, sep='\t', header=0, index_col=None, dtype=str)
#         pls[['Length_NUCCORE']] = pls[['Length_NUCCORE']].apply(pandas.to_numeric)
#         pls.set_index(keys='ACC_NUCCORE', drop=False, inplace=True, verify_integrity=True)
#         # linked samples
#         lsmp = pandas.read_csv(input.linked_samples, sep='\t', header=None, names=['UID_NUCCORE', 'UID_BIOSAMPLE'], dtype=str)
#         lsmp.set_index(keys='UID_NUCCORE', drop=True, inplace=True, verify_integrity=True)
#         # samples
#         smp = pandas.read_csv(input.samples, sep='\t', header=0, index_col=None, dtype=str)
#         smp.set_index(keys='UID_BIOSAMPLE', drop=True, inplace=True, verify_integrity=True)
#         # taxonomy
#         tax = pandas.read_csv(input.taxa, sep='\t', header=0, index_col=None, dtype=str)
#         # embedding
#         emb = pandas.read_csv(input.embedding, sep='\t', header=0, index_col='ID', dtype=str)
#         # PlasmidFinder
#         pf  = pandas.read_csv(input.plasmidfinder, sep='\t', header=0)
#         pfs = []
#         for aggr_id, aggr_df in pf.groupby(['qseqid']):
#             pfs.append({
#                 'qseqid': aggr_id,
#                 'plasmidfinder': '|'.join(aggr_df['sseqid'].values)
#             })
#         pfs = pandas.DataFrame(pfs)
#         pfs.set_index(keys='qseqid', drop=True, inplace=True, verify_integrity=True)
#         # pMLST
#         pmlst = pandas.read_csv(input.pmlst, sep='\t', header=None, index_col=0, names=['ID', 'pmlst'])
#
#         # add info from FASTA
#         logging.info('Adding info from FASTA {}'.format(input.fasta))
#         pls['ACC_FASTA'] = None
#         pls['GC_NUCCORE'] = None
#         with open(input.fasta, 'r') as ifile:
#             for record in tqdm(SeqIO.parse(ifile, 'fasta')):
#                 assert len(record.seq) > 0, 'Emptye sequence for record {}'.format(record.id)
#                 assert re.fullmatch(r'\w+\.\d+', record.id), 'Unexpected ID format in FASTA: {}'.format(record.id)
#                 rid = record.id.split('.')[0]
#                 assert rid in pls.index.values, 'FASTA record ID {}: not in plasmid table'.format(rid)
#                 assert pls.loc[rid,'GC_NUCCORE'] is None, 'FASTA record ID {}: GC value already set'.format(rid)
#                 assert len(record.seq) == pls.loc[rid,'Length_NUCCORE'], 'FASTA record ID {}: {} vs {} as length'.format(rid, len(record.seq), pls.loc[rid,'Length_NUCCORE'])
#                 pls.loc[rid,'ACC_FASTA'] = record.id
#                 pls.loc[rid,'GC_NUCCORE'] = GC(str(record.seq))
#
#         # add info from other tables
#         logging.info('Adding info from {}'.format(input.linked_samples))
#         pls = pandas.merge(
#             left=pls,
#             right=lsmp,
#             how='left',
#             left_on='UID_NUCCORE',
#             right_index=True,
#             sort=False,
#         )
#         logging.info('Adding info from {}'.format(input.samples))
#         pls = pandas.merge(
#             left=pls,
#             right=smp,
#             how='left',
#             left_on='UID_BIOSAMPLE',
#             right_index=True,
#             sort=False,
#         )
#         logging.info('Adding info from {}'.format(input.taxa))
#         pls = pandas.merge(
#             left=pls,
#             right=tax,
#             how='left',
#             left_on='TaxonID_NUCCORE',
#             right_on='taxon_id',
#             sort=False,
#         )
#         logging.info('Adding info from {}'.format(input.embedding))
#         pls = pandas.merge(
#             left=pls,
#             right=emb,
#             how='left',
#             left_on='ACC_FASTA',
#             right_index=True,
#             sort=False,
#         )
#         logging.info('Adding info from {}'.format(input.plasmidfinder))
#         pls = pandas.merge(
#             left=pls,
#             right=pfs,
#             how='left',
#             left_on='ACC_FASTA',
#             right_index=True,
#             sort=False,
#         )
#         logging.info('Adding info from {}'.format(input.pmlst))
#         pls = pandas.merge(
#             left=pls,
#             right=pmlst,
#             how='left',
#             left_on='ACC_FASTA',
#             right_index=True,
#             sort=False,
#         )
#
#         # drop not needed columns
#         pls.drop(columns=['TaxonID_NUCCORE'], inplace=True)
#
#         # checks
#         assert all(pandas.notnull(pls['ACC_FASTA']))
#         assert all(pandas.notnull(pls['Length_NUCCORE']))
#         assert all(pandas.notnull(pls['GC_NUCCORE']))
#
#         # save
#         pls.to_csv(
#             path_or_buf=output[0],
#             sep='\t',
#             header=True,
#             index=False,
#             index_label=False
#         )
#
# # Krona plot
# ##################################################
# rule krona_xml:
#     input:
#         MASTER_TAB
#     output:
#         MASTER_KRONA_XML
#     message:
#         "Create XML file for Krona plot from {input}"
#     params:
#         labels=' '.join(dbs.values())
#     shell:
#         "python create_krona_xml.py -t {input} -o {output} -l {params.labels}"
#
# rule krona_html:
#     input:
#         MASTER_KRONA_XML
#     output:
#         MASTER_KRONA_HTML
#     message:
#         "Create Krona plot from {input}"
#     params:
#         bin=BIN_KRONA_XML
#     shell:
#         "{params.bin} {input} -o {output}"
#
# # Other plots
# ##################################################
# rule other_plots:
#     input:
#         MASTER_TAB
#     output:
#         MASTER_PLOTS
#     message:
#         "Plotting stat.s from collected data"
#     params:
#         width=10,
#         height=6,
#         pattern=today
#     shell:
#         "Rscript create_plots.R --path $(dirname {input[0]}) --pattern {params.pattern} --pdf {output} --width {params.width} --height {params.height}"
