##################################################
# Config
##################################################
CONFIG_JSON = "pipeline.json"
configfile: CONFIG_JSON

##################################################
# Python modules
##################################################
import os
import re
import pandas
import pickle
import logging
import datetime
from tqdm import tqdm
from Bio import SeqIO
from collections import OrderedDict

from utils import setup_logger, run_cmd, mkdir

##################################################
# Some settings
##################################################

# Date
#today = datetime.datetime.now().strftime("%Y_%m_%d")
today = '2020_03_04'

# NCBI DBs
dbs = OrderedDict()
dbs['insd']   = 'INSDC'
dbs['refseq'] = 'RefSeq'

# conda path
tmp_c, tmp_s, CONDA_PATH = run_cmd('conda info --base')
assert tmp_s == 0
CONDA_PATH = CONDA_PATH.rstrip()

# conda env path
ENV_PATH = os.path.join(CONDA_PATH, 'envs', config['env_name'])

##################################################
# Tools
##################################################
# BLAST
BIN_MAKEBLASTDB = os.path.join(config["blast"]["dir"], "bin/makeblastdb")
BIN_BLASTN = os.path.join(config["blast"]["dir"], "bin/blastn")

# Mash
BIN_MASH = os.path.join(config["mash"]["dir"], "mash")

# edirect/eutils
EUTILS_INSTALL = os.path.join(config["eutils"]["dir"], "install.done")

# Krona
BIN_KRONA_XML = os.path.join(config["krona"]["dir"], "bin/ktImportXML")

# data for rMLST
RMLST_FASTA = config['rmlst']['rmlst_fas']
RMLST_DB = ['{}.{}'.format(RMLST_FASTA, ext) for ext in config['blast']['ext']]

# data for ABRicate
ABRICATE_UPDATE = 'data/abricate_update_%s.done' % today

# data for pMLST
PMLST_UPDATE  = 'data/pmlst_update_%s.done' % today
PMLST_CLEAN   = os.path.join(ENV_PATH, 'db/pubmlst/cleanup.done')
PMLST_ALLELES = os.path.join(ENV_PATH, 'db/pubmlst/download.done')
PMLST_DB      = os.path.join(ENV_PATH, 'db/blast/mlst.fa')

##################################################
# Data
##################################################
# Table with plasmid records
PLASMIDS                = [os.path.join(config['data']['odir']['reports'], '%s__%s__nuccore.txt' % (today, db)) for db in dbs.keys()]
PLASMIDS_ALL            = os.path.join(config['data']['odir']['reports'], '%s__nuccore.txt' % today)
# linked assemblies
PLASMIDS_ALL_ASM        = os.path.join(config['data']['odir']['reports'], '%s__nuccore_assembly.txt' % today)
# linked biosamples
PLASMIDS_ALL_BIOS       = os.path.join(config['data']['odir']['reports'], '%s__nuccore_biosample.txt' % today)
# assembly info
PLASMIDS_ASM            = os.path.join(config['data']['odir']['reports'], '%s__assembly.txt' % today)
# biosample info
PLASMIDS_BIOS           = os.path.join(config['data']['odir']['reports'], '%s__biosample.txt' % today)
# taxonomy info (processed)
PLASMIDS_TAX            = os.path.join(config['data']['odir']['reports'], '%s__taxa.txt' % today)
PLASMIDS_TAXP           = os.path.join(config['data']['odir']['reports'], '%s__taxaproc.txt' % today)
# all info together
PLASMIDS_FULL1          = os.path.join(config['data']['odir']['reports'], '%s__full1.txt' % today)
PLASMIDS_FULL2          = os.path.join(config['data']['odir']['reports'], '%s__full2.txt' % today)

# filter1
PLASMIDS_FILT1          = os.path.join(config['data']['odir']['reports'], '%s__filtered1.txt' % today)

# to find identical plasmids
PLASMIDS_FILT1_FASTA    = os.path.join(config['data']['odir']['reports'], '%s__filtered1.fna' % today)
PLASMIDS_FILT1_SKETCH   = os.path.join(config['data']['odir']['reports'], '%s__filtered1.msh' % today)
PLASMIDS_FILT1_DIST0    = os.path.join(config['data']['odir']['reports'], '%s__filtered1.dist0' % today)

# filter 2
PLASMIDS_FILT2          = os.path.join(config['data']['odir']['reports'], '%s__filtered2.txt' % today)

# to find putative chromosomes
PLASMIDS_FILT2_FASTA    = os.path.join(config['data']['odir']['reports'], '%s__filtered2.fna' % today)
PLASMIDS_FILT2_RMLST    = os.path.join(config['data']['odir']['reports'], '%s__filtered2.rmlst' % today)

# filter 3
PLASMIDS_FILT3          = os.path.join(config['data']['odir']['reports'], '%s__filtered3.txt' % today)

##################################################
# Data: "Master" files
##################################################
# Plasmid sequences
MASTER_FASTA        = os.path.join(config["data"]["odir"]["master"], "%s.fna" % today)

# ABRicate
MASTER_ABRICATE_DBS = expand(os.path.join(config["data"]["odir"]["master"], "%s.abr.{rdb}" % today), rdb=config['abricate']['dbs'])
MASTER_ABRICATE     = os.path.join(config["data"]["odir"]["master"], "%s.abr" % today)

# pMLST
MASTER_PMLST        = os.path.join(config["data"]["odir"]["master"], "%s.pmlst" % today)

# BLASTdb from master FASTA
MASTER_BLASTDB      = expand(os.path.join(config["data"]["odir"]["master"], "%s.fna.{ext}" % today), ext=config['blast']['ext'])

# Mash
MASTER_MASH_SIG     = os.path.join(config["data"]["odir"]["master"], "%s.msh"  % today)
MASTER_MASH_DIST    = os.path.join(config["data"]["odir"]["master"], "%s.dist" % today)
MASTER_MASH_DISTS   = os.path.join(config["data"]["odir"]["master"], "%s.distS" % today)

# Similar records
MASTER_SIM          = os.path.join(config["data"]["odir"]["master"], "%s.sim" % today)

# UMAP
MASTER_MASH_UMAP    = os.path.join(config["data"]["odir"]["master"], "%s.umap" % today)

# Master info table
MASTER_TAB          = os.path.join(config["data"]["odir"]["master"], "%s.tsv" % today)

# Krona plot
MASTER_KRONA_XML    = os.path.join(config["data"]["odir"]["master"], "%s.xml" % today)
MASTER_KRONA_HTML   = os.path.join(config["data"]["odir"]["master"], "%s.html" % today)

# Other plots
MASTER_SUM_PLOTS    = os.path.join(config["data"]["odir"]["master"], "%s_summary.pdf" % today)
MASTER_SUM_STATS    = os.path.join(config["data"]["odir"]["master"], "%s_summary.txt" % today)

# Compare created table to an old version
MASTER_CHANGES      = os.path.join(config["data"]["odir"]["master"], "%s_changes.tsv" % today)

##################################################
# Rules
##################################################

# ALL rule
##################################################
rule all:
    input:
        # NOTE Should be executed on the same day
        # tools
        blast=[BIN_MAKEBLASTDB, BIN_BLASTN],
        mash=BIN_MASH,
        eutils=EUTILS_INSTALL,
        krona=BIN_KRONA_XML,
        pmlstdb=[PMLST_UPDATE, PMLST_DB],
        abricate=ABRICATE_UPDATE,
        rmlst=RMLST_DB, # NOTE Requires an existing FASTA (see README.md)
        # plasmids
        query_plasmids=PLASMIDS_ALL,
        query_assemblies=[PLASMIDS_ALL_ASM, PLASMIDS_ASM],
        query_biosamples=[PLASMIDS_ALL_BIOS,PLASMIDS_BIOS],
        query_taxa=[PLASMIDS_TAX, PLASMIDS_TAXP],
        query_plasmids2=PLASMIDS_FULL2,
        # plasmid filtering (1)
        filter_plasmids1=PLASMIDS_FILT1,
        # plasmid filtering (2)
        filter_plasmids2=PLASMIDS_FILT2,
        # rmlst
        run_rmlst=PLASMIDS_FILT2_RMLST, # NOTE One of the most time consuming steps (ca. 6 hours for ca. 17k records, 07.10.19: > 12h. Takes a lot of time!)
        # plasmid filtering (3)
        filter_plasmids3=PLASMIDS_FILT3, # NOTE One of the most time consuming steps (multiple hours, depends on the number of sequences)
        # NOTE Can be executed on other days (don't forget to set "today" to a fixed value)
        # master files
        # fasta
        master_fasta=MASTER_FASTA,
        # abricate
        master_abricate=MASTER_ABRICATE_DBS + [MASTER_ABRICATE], # NOTE Do not execute these rules in parallel (access to same tmp files) (around 2h in my last run)
        # pmlst
        pmlst=MASTER_PMLST,
        # blastdb
        master_blastdb=MASTER_BLASTDB,
        # mash
        master_sig=MASTER_MASH_SIG,
        master_dist=[MASTER_MASH_DIST, MASTER_MASH_DISTS],
        # similar records
        master_sim=MASTER_SIM,
        # umap
        master_umap=MASTER_MASH_UMAP,
        # table
        master_tab=MASTER_TAB,
        # krona plot
        master_krona=[MASTER_KRONA_XML, MASTER_KRONA_HTML],
        # other plots
        master_summary=[MASTER_SUM_PLOTS, MASTER_SUM_STATS],
        # compare to older version
        master_compare=MASTER_CHANGES

# Tools
##################################################
# blast
rule install_blast:
    output:
        BIN_MAKEBLASTDB, BIN_BLASTN
    params:
        url=config["blast"]["url"],
        name=os.path.basename(config["blast"]["url"])
    message:
        "installing blast from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xzvf {params.name} && rm {params.name}"

# mash
rule install_mash:
    output:
        BIN_MASH
    params:
        url=config["mash"]["url"],
        name=os.path.basename(config["mash"]["url"])
    message:
        "installing mash from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xvf {params.name} && rm {params.name}"

# edirect/eutils
rule install_eutils:
    output:
        EUTILS_INSTALL
    message:
        "Install edirect/euils"
    shell:
        """
        mkdir -p $(dirname {output}) && cd tools
        perl -MNet::FTP -e \
            '$ftp = new Net::FTP("ftp.ncbi.nlm.nih.gov", Passive => 1);
             $ftp->login; $ftp->binary;
             $ftp->get("/entrez/entrezdirect/edirect.tar.gz");'
        gunzip -c edirect.tar.gz | tar xf -
        rm edirect.tar.gz
        cd edirect && wget https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/xtract.Linux.gz && gunzip -f xtract.Linux.gz && chmod +x xtract.Linux
        touch $(basename {output})
        """

# krona
rule install_krona:
    output:
        BIN_KRONA_XML
    message:
        "Install Krona"
    params:
        url=config["krona"]["url"],
        name=os.path.basename(config["krona"]["url"])
    shell:
        """
        mkdir -p tools && cd tools && wget {params.url} && unzip {params.name} && rm {params.name} &&
        rsync -av Krona-xl2.5/KronaTools . && rm -r Krona-xl2.5/ && cd KronaTools && ./install.pl --prefix .
        """

# abricate: data
rule update_abricate:
    output:
        ABRICATE_UPDATE
    params:
        dbs=config['abricate']['dbs']
    message:
        "Update ABRicate databases: {params.dbs}"
    run:
        logger = setup_logger(logging.INFO)

        for db in params.dbs:
            # run ABRicate to update the databases
            cmd = "abricate-get_db --db {db} --force".format(db=db)
            logger.info('Update with ABRicate DB {}: {}'.format(db, cmd))
            cmd, cmd_s, cmd_o = run_cmd(cmd)
            assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)
            with open(output[0], 'a') as ofile:
            	ofile.write(cmd_o)

            # reformat FASTA: remove non-UTF-8 characters
            seq0 = "{env_path}/db/{db}/sequence"
            seq1 = seq0 + ".tmp"
            cmd = "mv {input} {input}.tmp && iconv -c -t UTF-8 < {input}.tmp > {input}".format(
                input=os.path.join(ENV_PATH, 'db', db, 'sequences')
            )
            logger.info('Remove non-UTF-8 symbols: {}'.format(cmd))
            cmd, cmd_s, cmd_o = run_cmd(cmd)
            assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)

        with open(output[0], 'a') as ofile:
            ofile.write('\nUPDATE DONE.\n')

# mlst
# pmlst: data
rule update_pmlst:
    output:
        PMLST_UPDATE
    shell:
        "touch {output}"

rule clean_pmlst:
    input:
        PMLST_UPDATE
    output:
        PMLST_CLEAN
    message:
        "Removing all schemes from mlst db"
    params:
        dir=ENV_PATH
    shell:
        """
        find {params.dir}/db/pubmlst/ -maxdepth 1 -mindepth 1 -type d -exec rm -r {{}} \; && \
        find {params.dir}/db/blast/ -type f -name 'mlst.*' -exec rm {{}} \; && \
        touch {output}
        """

rule get_pmlst_data:
    input:
        PMLST_CLEAN
    output:
        PMLST_ALLELES
    message:
        "Download pMLST allele FASTA files"
    params:
        db=config['pmlst']['db'],
        url=config['pmlst']['url'],
        url_schemes=config['pmlst']['url_schemes'],
    run:
        # reference: https://github.com/kjolley/BIGSdb.git: scripts/rest_examples/python/download_alleles.py
        import requests
        from glob import glob
        from utils import proc_mlst_scheme_name, download_pmlst_scheme_alleles, download_pmlst_scheme_profiles

        logger = setup_logger(logging.INFO)

        # get schemes
        schemes = requests.get(params.url_schemes)
        assert schemes.status_code != 404, 'Invalid URL {}'.format(params.url_schemes)
        schemes = schemes.json()['schemes']
        logger.info('There are {} pMLST schemes'.format(len(schemes)))

        # get loci
        for scheme in schemes:
            # scheme name and URL
            scheme_name = scheme['description']
            scheme_url  = scheme['scheme']
            scheme_name2 = proc_mlst_scheme_name(scheme_name)
            logger.info('Scheme {} ({}): {}'.format(scheme_name, scheme_name2, scheme_url))

            # where to save files
            scheme_dir = os.path.join(os.path.dirname(output[0]), scheme_name2)
            mkdir(scheme_dir, True)

            # where to save profiles
            scheme_profiles = os.path.join(scheme_dir, scheme_name2 + '.txt')

            # alleles
            download_pmlst_scheme_alleles(scheme_name, scheme_url, scheme_dir)

            # profiles
            profiles_url = params.url +  '/db/' + params.db + '/schemes/' + scheme_url.split('/')[-1]
            download_pmlst_scheme_profiles(scheme_name, profiles_url, scheme_dir, scheme_profiles)

        with open(output[0], 'w') as ofile:
            ofile.write('done')

rule create_pmlst_db:
    input:
        PMLST_ALLELES
    output:
        PMLST_DB
    message:
        "Creating pMLST db"
    params:
        dir=ENV_PATH
    shell:
        "{params.dir}/scripts/mlst-make_blast_db"

# rmlst: data
# NOTE: FASTA file must exist
rule rmlst_blastdb:
    input:
        fasta=RMLST_FASTA,
        bin=BIN_MAKEBLASTDB
    output:
        RMLST_DB
    message:
        "Create BLASTDB from {input}"
    shell:
        "{input.bin} -in {input.fasta} -input_type fasta -dbtype nucl -title \"rmlst\""

# Plasmid query
##################################################
rule query_plasmids:
    input:
        EUTILS_INSTALL
    output:
        "{dir}/{date}__{db}__nuccore.txt"
    message:
        "Query for plasmids in {wildcards.db}"
    params:
        query=lambda wildcards: config['eutils']['query']['plasmid']['cmd'].format(
            path=config['eutils']['dir'],
            esearch_query=config['eutils']['query']['plasmid']['esearch_query'],
            efilter_query=config['eutils']['query']['plasmid']['efilter_query'].format(db=wildcards.db),
            xtract_query=config['eutils']['query']['plasmid']['xtract_query']
        ),
        header='\t'.join(config['eutils']['header']['plasmid'])
    shell:
        """
        mkdir -p $(dirname {output}) && echo -e \"{params.header}\" > {output} && {params.query} >> {output}
        """

rule queried_plasmids:
    input:
        PLASMIDS
    output:
        PLASMIDS_ALL
    message:
        "Creating a table of all found plasmids"
    run:
        logger = setup_logger(logging.INFO)

        dfs = []
        for df_file in input:
            df = pandas.read_csv(df_file, sep='\t', header=0)
            df_source = df_file.split('__')[1]
            assert df_source in dbs.keys(), 'Unknown source \"{}\"'.format(df_source)
            df['Source_NUCCORE'] = df_source
            dfs.append(df)
        dfs = pandas.concat(dfs)
        dfs.to_csv(output[0], sep='\t', index=False, header=True)

# Linked assemblies
rule query_linked_assemblies:
    input:
        PLASMIDS_ALL
    output:
        PLASMIDS_ALL_ASM
    message:
        "Query for linked Assemblies from {input}"
    run:
        from utils import run_epost_split
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['link_asm']),
            cmd=config['eutils']['query']['linked'],
            df_col='UID_NUCCORE',
            split_size=200,
            split_str=None,
            path=config['eutils']['dir'],
            db_source='nuccore',
            db_target='assembly'
        )

rule query_assemblies:
    input:
        PLASMIDS_ALL_ASM
    output:
        PLASMIDS_ASM
    message:
        "Query for Assemblies from {input}"
    run:
        from utils import run_epost_split
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['assembly']),
            cmd=config['eutils']['query']['assembly'],
            df_col='UID_ASSEMBLY',
            split_size=200,
            split_str=';',
            path=config['eutils']['dir']
        )

# Linked biosamples
rule query_linked_biosamples:
    input:
        PLASMIDS_ALL
    output:
        PLASMIDS_ALL_BIOS
    message:
        "Query for linked BioSamples from {input}"
    run:
        from utils import run_epost_split
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['link_bios']),
            cmd=config['eutils']['query']['linked'],
            df_col='UID_NUCCORE',
            split_size=200,
            split_str=None,
            path=config['eutils']['dir'],
            db_source='nuccore',
            db_target='biosample'
        )

rule query_biosamples:
    input:
        PLASMIDS_ALL_BIOS
    output:
        PLASMIDS_BIOS
    message:
        "Query for BioSamples from {input}"
    run:
        from utils import run_epost_split
        run_epost_split(
            df_file=input[0],
            ofile=output[0],
            header='\t'.join(config['eutils']['header']['biosample']),
            cmd=config['eutils']['query']['biosample'],
            df_col='UID_BIOSAMPLE',
            split_size=300,
            split_str=';',
            path=config['eutils']['dir']
        )

# Linked taxonomy
rule query_taxon:
    input:
        PLASMIDS_ALL
    output:
        PLASMIDS_TAX
    message:
        "Query for linked Taxa from {input}"
    run:
        logger = setup_logger(logging.INFO)

        ids = pandas.read_csv(input[0], sep='\t', header=0, dtype=str)['TaxonID_NUCCORE'].fillna(value='')
        ids = set(ids.values)
        assert '' not in ids, 'Have an empty ID in {}'.format(input[0])
        logger.info('There are {} unique taxonomy IDs'.format(len(ids)))

        cmd = "{query} > {ofile}".format(
            query=config['eutils']['query']['taxon'].format(
                path=config['eutils']['dir'],
                ids=','.join(ids)
            ),
            ofile=output[0]
        )
        cmd, cmd_s, cmd_o = run_cmd(cmd)
        assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)

rule proc_taxon:
    input:
        PLASMIDS_TAX
    output:
        PLASMIDS_TAXP
    message:
        "Process taxa from {input}"
    run:
        logger = setup_logger(logging.INFO)

        header = config['eutils']['header']['taxon']
        ranks  = config['eutils']['header']['ranks']
        for rank in ranks:
            header += ['taxon_%s_id' % rank, 'taxon_%s_name' % rank]

        df = pandas.DataFrame(columns=header)
        with open(input[0], 'r') as ifile:
            i = 0
            for line in tqdm(ifile):
                qids  = []
                qinfo = None
                line = line.rstrip('\n')
                for field in line.split('\t'):
                    if re.search('\|',field):
                        fields = field.split('|')

                        # taxon ID, name, rank
                        tid   = None
                        tname = fields[-2]
                        trank = fields[-1]
                        if trank == 'no rank':
                            trank = 'NA'

                        # taxon ID
                        if len(qids) == 0: # query ID, save all
                            for f_i, f_n in enumerate(fields):
                                if re.fullmatch(r'\d+', f_n) and f_i < (len(fields) - 2):
                                    qids.append(f_n)
                            if len(qids) > 1:
                                logger.info('Taxon with multiple IDs: {}'.format(field))
                            qinfo = dict.fromkeys(qids)
                            for qid in qids:
                                qinfo[qid] = dict.fromkeys(header, 'NA')
                                qinfo[qid]['taxon_id'] = qid
                                qinfo[qid]['taxon_name'] = tname
                                qinfo[qid]['taxon_rank'] = trank
                        else: # ignore mult. IDs, take first one
                            tid = fields[0]

                        # if relevant rank
                        assert qinfo is not None
                        if 'taxon_%s_name' % trank in header:
                            for qid in qids:
                                if tid is None:
                                    qinfo[qid]['taxon_%s_id' % trank] = qid
                                else:
                                    qinfo[qid]['taxon_%s_id' % trank] = tid
                                qinfo[qid]['taxon_%s_name' % trank] = tname
                                qinfo[qid]['taxon_%s_rank' % trank] = trank
                    else: # lineage
                        assert qinfo is not None
                        for qid in qids:
                            qinfo[qid]['lineage'] = field
                # save info for each query
                for qid in qids:
                    df.loc[qid] = [qinfo[qid][k] for k in header]
            # replace nulls by 'NA'
            df.fillna(value='NA', inplace=True)
            # keep only unqiue rows
            df.drop_duplicates(inplace=True)
            # save
            df.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Add to table
rule collect_meta:
    input:
        pls=PLASMIDS_ALL,
        asm=PLASMIDS_ASM,
        bios=PLASMIDS_BIOS,
        tax=PLASMIDS_TAXP,
        pls_asm=PLASMIDS_ALL_ASM,
        pls_bios=PLASMIDS_ALL_BIOS,
    output:
        PLASMIDS_FULL1
    message:
        "Adding meta info to {input.pls}"
    run:
        import datetime

        logger = setup_logger(logging.INFO)

        pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
        pls.set_index('UID_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid records\n{}'.format(pls.shape[0], pls.head()))

        asm = pandas.read_csv(input.asm, sep='\t', header=0, dtype=str)
        asm.set_index(asm.UID_ASSEMBLY, drop=True, inplace=True, verify_integrity=True)
        logger.info('Read in {} assembly records\n{}'.format(asm.shape[0], asm.head()))

        tax = pandas.read_csv(input.tax, sep='\t', header=0, dtype=str)
        tax.set_index('taxon_id', drop=True, inplace=True, verify_integrity=True)
        logger.info('Read in {} taxonomy records\n{}'.format(tax.shape[0], tax.head()))

        bios = pandas.read_csv(input.bios, sep='\t', header=0, dtype=str)
        bios.set_index('UID_BIOSAMPLE', drop=True, inplace=True, verify_integrity=True)
        logger.info('Read in {} biosample records\n{}'.format(bios.shape[0], bios.head()))

        pls_asm = pandas.read_csv(input.pls_asm, sep='\t', header=0, dtype=str)
        pls_asm.set_index('UID_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid/assembly records\n{}'.format(pls_asm.shape[0], pls_asm.head()))

        pls_bios = pandas.read_csv(input.pls_bios, sep='\t', header=0, dtype=str)
        pls_bios.set_index('UID_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid/assembly records\n{}'.format(pls_bios.shape[0], pls_bios.head()))

        # process plasmid/assembly mapping - only latest or none
        def get_latest_assembly(uids):
            if pandas.isnull(uids):
                return None
            uids = uids.split(';')
            if len(uids) == 1:
                assert uids[0] in asm.index, 'Unknown UID, check assembly.txt and nuccore_assembly.txt and consider removal and rerun for assembly.txt: {}'.format(uids[0])
                return uids[0]
            for uid in uids:
                assert uid in asm.index, 'Unknown UID, check assembly.txt and nuccore_assembly.txt and consider removal and rerun for assembly.txt: {}'.format(uid)
                if asm.loc[uid,'Latest_ASSEMBLY'] == 'True':
                    return uid
            # get dates, parse, sort, get first
            dates = asm.loc[uids,'SeqReleaseDate_ASSEMBLY'].map(lambda x: datetime.datetime.strptime(x, '%Y/%m/%d %H:%M'))
            dates.sort_values(ascending=False, inplace=True)
            logger.warn('No latest assembly among {}: latest w.r.t. date is {}'.format(uids, dates.index[0]))
            return dates.index[0]

        #for uid in pls_asm['UID_ASSEMBLY']:
        #    assert uid in asm['UID_ASSEMBLY'], 'Unknown UID {}'.format(uid)
        pls_asm['UID_ASSEMBLY'] = pls_asm['UID_ASSEMBLY'].map(get_latest_assembly)

        # add assembly ID
        pls = pandas.merge(
            left=pls,
            right=pls_asm,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )

        # add assembly info
        pls = pandas.merge(
            left=pls,
            right=asm,
            how='left',
            left_on='UID_ASSEMBLY',
            right_index=True,
            sort=False,
        )

        # add biosample ID
        pls = pandas.merge(
            left=pls,
            right=pls_bios,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )

        # add biosample info
        pls = pandas.merge(
            left=pls,
            right=bios,
            how='left',
            left_on='UID_BIOSAMPLE',
            right_index=True,
            sort=False,
        )

        # add taxonomy
        pls = pandas.merge(
            left=pls,
            right=tax,
            how='left',
            left_on='TaxonID_NUCCORE',
            right_index=True,
            sort=False,
        )

        # check if this column name is really used
        if 'UID_ASSEMBLY_x' in pls:
            print("Remove duplicate column")
            # check if columns are really identical
            assert pls['UID_ASSEMBLY_x'].equals(pls['UID_ASSEMBLY_y']), 'Unknown UID contained'
            # merge the two identical columns (else pipe gets problems later on)
            pls = pls.rename(columns={"UID_ASSEMBLY_x": "UID_ASSEMBLY"})
            pls = pls.drop(columns="UID_ASSEMBLY_y")        

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Locations
rule parse_locations:
    input:
        PLASMIDS_FULL1
    output:
        PLASMIDS_FULL2
    message:
        "Parsing locations from {input}"
    params:
        api_keys=config['data']['api_keys'],
        locs=config['data']['locs']
    run:
        from utils import preproc_loc_str, preproc_loc_coords, parse_location
        from utils import load_locs, save_locs, update_locs

        logger = setup_logger(logging.INFO)

        # api key
        print("key")
        with open(params.api_keys, 'r') as ifile:
            api_key = [k.rstrip('\n').strip() for k in ifile.readlines()]
            api_key = api_key.pop()
        logger.info('Using API key: {}'.format(api_key))

        print("locations")
        # load known locations
        locs = load_locs(params.locs)

        # sample table with locations
        pls = pandas.read_csv(input[0], header=0, sep='\t', dtype=str)
        pls.set_index('UID_NUCCORE', drop=False, inplace=True, verify_integrity=True)

        # parse
        coords = []
        for i in tqdm(pls.index):
            # location name and coordinates
            l_n = pls.loc[i,'Location_BIOSAMPLE']
            l_c = pls.loc[i,'Coordinates_BIOSAMPLE']

            # no location data
            if l_n is None and l_c is None:
                continue

            # pre-processing
            l_n = preproc_loc_str(l_n)
            l_c = preproc_loc_coords(preproc_loc_str(l_c))
            if l_n is None and l_c is None:
                continue

            # at least name or coordinates given
            if l_c is not None: # location coordinates
                l_c_str = '{};{}'.format(l_c[0], l_c[1])
                if locs is None or l_c_str not in locs['location']:
                    logger.info('Retrieving coordinates for location: \"{}\"'.format(l_c))
                    try:
                        parsed = parse_location(loc_str=l_c, api_key=api_key, is_name=False)
                        locs = update_locs(locs, {'location': l_c_str, 'type': 'coordinates', 'lat': parsed['lat'], 'lng': parsed['lng']})
                        save_locs(locs, params.locs)
                    except Exception as e:
                        logger.error('Error while retrieving coordinates for location \"{}\"'.format(l_c))
                        raise(e)
                coords.append({'ID': i, 'loc_lat': locs.loc[l_c_str,'lat'], 'loc_lng': locs.loc[l_c_str,'lng']})
            elif l_n is not None: # location name
                if locs is None or l_n not in locs['location']:
                    logger.info('Retrieving coordinates for location: \"{}\"'.format(l_n))
                    try:
                        parsed = parse_location(loc_str=l_n, api_key=api_key, is_name=True)
                        locs = update_locs(locs, {'location': l_n, 'type': 'name', 'lat': parsed['lat'], 'lng': parsed['lng']})
                        save_locs(locs, params.locs)
                    except Exception as e:
                        logger.error('Error while retrieving coordinates for location \"{}\"'.format(l_n))
                        raise(e)
                coords.append({'ID': i, 'loc_lat': locs.loc[l_n,'lat'], 'loc_lng': locs.loc[l_n,'lng'], 'loc_parsed': l_n})
        coords = pandas.DataFrame(coords)
        coords.set_index('ID', drop=True, verify_integrity=True, inplace=True)
        pls = pandas.merge(
            left=pls,
            right=coords,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# Plasmid filtering (1)
##################################################
rule filter1:
    input:
        PLASMIDS_FULL2
    output:
        PLASMIDS_FILT1
    message:
        "Filter plasmids from {input}"
    params:
        dfilter=config['filtering']['dfilter']
    run:
        logger = setup_logger(logging.INFO)
        print(input[0])
        pls = pandas.read_csv(input[0], sep='\t', header=0, dtype=str)
        pls.set_index('UID_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid records'.format(pls.shape[0]))

        # filter by description
        keep_rows = pandas.Series([re.search(params.dfilter, d, flags=re.IGNORECASE) is None for d in pls['Description_NUCCORE']], index=pls.index)
        pls = pls.loc[keep_rows,:]
        logger.info('Filtered: description "{}": kept {} records'.format(params.dfilter, pls.shape[0]))

        # filter by (assembly) completeness
        def pls_filter(uid):

            assembly_tag = pls.loc[uid, 'Status_ASSEMBLY']

            has_assembly_tag = pandas.notnull(pls.loc[uid, 'UID_ASSEMBLY']) and pandas.notnull(assembly_tag) and assembly_tag != ""

            complete_tag  = pls.loc[uid, 'Completeness_NUCCORE']

            has_complete_tag = pandas.notnull(complete_tag) and complete_tag != ""

            if has_assembly_tag and has_complete_tag:
                return assembly_tag == 'Complete Genome' and complete_tag == 'complete'
            elif not has_assembly_tag:
                return complete_tag == 'complete'
            elif not has_complete_tag:
                return assembly_tag == 'Complete Genome'
            return False

        logger.info('Completeness nuccore tag: {}'.format(set(pls['Completeness_NUCCORE'])))
        logger.info('Completeness assembly tag: {}'.format(set(pls['Status_ASSEMBLY'])))
        print("try index map")
        print(pls.index)
        pls = pls.loc[pls.index.map(pls_filter),:]
        print("succeeded index map")
        logger.info('Filtered: (assembly) completeness: kept {} records'.format(pls.shape[0]))

        # filter by taxonomy
        pls = pls.loc[pls['taxon_superkingdom_id'] == '2',:]
        logger.info('Filtered: superkingdom ID is 2 (i.e. Bacteria): kept {} records'.format(pls.shape[0]))

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False)

# Plasmid filtering (2)
##################################################
rule filter2:
    input:
        pls=PLASMIDS_FILT1,
        fna=PLASMIDS_FILT1_FASTA,
        dist=PLASMIDS_FILT1_DIST0
    output:
        PLASMIDS_FILT2
    message:
        "Filter out identical plasmids {input.pls}"
    run:
        import numpy
        from Bio import SeqIO
        from Bio.SeqUtils import GC
        from utils import str2timestamp

        logger = setup_logger(logging.INFO)

        pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
        pls.set_index('ACC_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid records'.format(pls.shape[0]))

        # collect seq. stats.s
        logger.info('Collecting sequences and their data ...')
        seq = []
        seqs = {}
        with open(input.fna, 'r') as ifile:
            for record in tqdm(SeqIO.parse(ifile, 'fasta')):
                assert re.fullmatch(r'\w+\.\d+', record.id), 'Unexpected ID format in FASTA: {}'.format(record.id)
                assert record.id in list(pls.index), 'Unknown FASTA ID {}'.format(record.id)
                seq.append({
                    'ACC_NUCCORE': record.id,
                    'GC_NUCCORE': GC(record.seq),
                    'Length': len(record.seq)
                })
                seqs[record.id] = record.seq
        seq = pandas.DataFrame(seq)
        seq.set_index('ACC_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        # add to table
        pls = pandas.merge(
            left=pls,
            right=seq,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        # check seq. length
        assert pls['Length_NUCCORE'].astype(int).equals(pls['Length'].astype(int)), 'Sequence length is not always identical: nuccore vs. FASTA'
        # drop 2nd seq. len. col.
        pls.drop(columns=['Length'], inplace=True)

        # find and assign identity groups
        logger.info('Grouping identical sequences ...')
        pls['UID_IDENTGROUP'] = None
        groupID = 0
        with open(input.dist, 'r') as ifile:
            for line in tqdm(ifile):
                sID, qID, dist, pv, sh = line.rstrip('\n').split('\t')
                # same ID -> skip
                if sID == qID:
                    continue
                # group ID already set -> skip
                if pandas.notnull(pls.loc[sID, 'UID_IDENTGROUP']) and pandas.notnull(pls.loc[qID, 'UID_IDENTGROUP']):
                    continue
                # check if really equal
                if len(seqs[sID]) != len(seqs[qID]):
                    continue
                elif seqs[sID] != seqs[qID]: # length is equal -> check sequences
                    continue
                # sequences are identical
                # one of the seq.s has already an ID -> set to same value
                if pandas.notnull(pls.loc[sID, 'UID_IDENTGROUP']):
                    pls.loc[qID, 'UID_IDENTGROUP'] = pls.loc[sID, 'UID_IDENTGROUP']
                    continue
                if pandas.notnull(pls.loc[qID, 'UID_IDENTGROUP']):
                    pls.loc[sID, 'UID_IDENTGROUP'] = pls.loc[qID, 'UID_IDENTGROUP']
                    continue
                # new group -> set both to new value
                groupID += 1
                pls.loc[sID, 'UID_IDENTGROUP'] = groupID
                pls.loc[qID, 'UID_IDENTGROUP'] = groupID
        # save
        pls.to_csv(output[0] + '.backup', sep='\t', header=True, index=False, index_label=False)

        # filter
        def compare_records(rec1, rec2):
            """
            Compare two plasmid records:
            0) Prefer the one with higher version number (if same accession)
            1) Prefer the one from RefSeq
            2) Prefer the one with location information
            3) Prefer the one with an assembly
            4) Prefer the older one
            5) Compare by accession (string), prefer the "smaller" one (just have something to break the ties)
            """
            rec1_acc, rec1_ver = rec1['ACC_NUCCORE'].split('.')
            rec1_ver = int(rec1_ver)
            rec2_acc, rec2_ver = rec2['ACC_NUCCORE'].split('.')
            rec2_ver = int(rec2_ver)

            if rec1_acc == rec2_acc:
                assert rec1_ver != rec2_ver, 'Both records have the same accession and version: {} vs {}'.format(rec1['UID_NUCCORE'], rec1['UID_NUCCORE'])
                if rec1_ver > rec2_ver:
                    return rec1
                else:
                    return rec2
            elif rec1['Source_NUCCORE'] == "refseq" and rec2['Source_NUCCORE'] != "refseq":
                return rec1
            elif rec1['Source_NUCCORE'] != "refseq" and rec2['Source_NUCCORE'] == "refseq":
                return rec2
            elif pandas.notnull(rec1['loc_lat']) and pandas.isnull(rec2['loc_lat']):
                return rec1
            elif pandas.isnull(rec1['loc_lat'])  and pandas.notnull(rec2['loc_lat']):
                return rec2
            elif pandas.notnull(rec1['UID_ASSEMBLY']) and pandas.isnull(rec2['UID_ASSEMBLY']):
                return rec1
            elif pandas.isnull(rec1['UID_ASSEMBLY'])  and pandas.notnull(rec2['UID_ASSEMBLY']):
                return rec2
            elif str2timestamp(rec1['CreateDate_NUCCORE'], '%Y/%m/%d') < str2timestamp(rec2['CreateDate_NUCCORE'], '%Y/%m/%d'):
                return rec1
            elif str2timestamp(rec1['CreateDate_NUCCORE'], '%Y/%m/%d') > str2timestamp(rec2['CreateDate_NUCCORE'], '%Y/%m/%d'):
                return rec2
            else:
                if rec1['ACC_NUCCORE'] < rec2['ACC_NUCCORE']:
                    return rec1
                else:
                    return rec2

        logger.info('Filter by identity ...')
        pls['Identical'] = None
        keep = pandas.Series(True, index=pls['ACC_NUCCORE'])
        for gr, gr_df in tqdm(pls.loc[pandas.notnull(pls['UID_IDENTGROUP']),:].groupby(by=['UID_IDENTGROUP'])):
            assert pandas.notnull(gr)
            best = None
            for acc in gr_df.index:
                if best is None:
                    best = acc
                else:
                    best = compare_records(gr_df.loc[best,:], gr_df.loc[acc,:])['ACC_NUCCORE']
            keep.loc[gr_df.loc[gr_df['ACC_NUCCORE'] != best,'ACC_NUCCORE']] = False
            pls.loc[best,'Identical'] = ';'.join(sorted([acc for acc in gr_df['ACC_NUCCORE'] if acc != best]))
        pls = pls.loc[keep.index[keep]]
        pls.drop(columns=['UID_IDENTGROUP'], inplace=True)
        logger.info('Filtered: indentical sequences: kept {} records'.format(pls.shape[0]))

        # by accession and version
        logger.info('Filter by accession ...')
        pls['ACC_WO_VERSION'] = pls['ACC_NUCCORE'].apply(lambda x: x.split('.')[0])
        pls['OldVersion'] = None
        keep = pandas.Series(True, index=pls['ACC_NUCCORE'])
        for gr, gr_df in tqdm(pls.groupby(by=['ACC_WO_VERSION'])):
            assert pandas.notnull(gr)
            best = None
            for acc in gr_df.index:
                if best is None:
                    best = acc
                else:
                    best = compare_records(gr_df.loc[best,:], gr_df.loc[acc,:])['ACC_NUCCORE']
            keep.loc[gr_df.loc[gr_df['ACC_NUCCORE'] != best,'ACC_NUCCORE']] = False
            pls.loc[best,'OldVersion'] = ';'.join(sorted([acc for acc in gr_df['ACC_NUCCORE'] if acc != best]))
        pls = pls.loc[keep.index[keep]]
        pls.drop(columns=['ACC_WO_VERSION'], inplace=True)
        logger.info('Filtered: sequences w/ same accession: kept {} records'.format(pls.shape[0]))

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

# rMLST
##################################################
# run rMLST
rule rmlst_blastn:
    input:
        fasta="{basename}.fna",
        db=RMLST_FASTA,
        dbs=RMLST_DB
    output:
        "{basename}.rmlst"
    params:
        bin=BIN_BLASTN,
        cores=30
    run:
        logger = setup_logger(logging.INFO)

        header = config['rmlst']['rmlst_header']
        ident  = config['rmlst']['rmlst_ident']
        cov    = config['rmlst']['rmlst_cov']

        # blastn search
        cmd = config['rmlst']['rmlst_cmd'].format(
            blastn=params.bin,
            input=input.fasta,
            db=input.db,
            output=output[0],
            ident=ident,
            header=' '.join(header),
            cores=params.cores
        )
        logger.info(cmd)
        cmd, cmd_s, cmd_o = run_cmd(cmd)
        assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)

        # read in results
        df = pandas.read_csv(output[0], sep='\t', header=None, names=header)

        # compute coverage
        df['cov'] =  100 * (df['length'] - df['gaps']) / df['slen']
        # filter by coverage
        df = df.loc[df['cov'] >= cov,:]

        # rMLST locus
        df['slocus'] = df['sseqid'].map(lambda x: x.split('_')[0])

        # save
        df.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )

# Plasmid filtering (3)
##################################################
rule filter3:
    input:
        pls=PLASMIDS_FILT2,
        fna=PLASMIDS_FILT2_FASTA,
        rmlst=PLASMIDS_FILT2_RMLST
    output:
        PLASMIDS_FILT3
    message:
        "Filter based on rMLST results"
    params:
        cores=30,
        cutoff=config['rmlst']['rmlst_max_loci'],
        blastn_bin=BIN_BLASTN,
        blastn_header=config['rmlst']['blastn_header'],
        blastn_pident=config['rmlst']['blastn_pident'],
        blastn_qcovs=config['rmlst']['blastn_qcovs']
    run:
        from multiprocessing import Pool

        logger = setup_logger(logging.INFO)

        pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
        pls.set_index('ACC_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid records\n{}'.format(pls.shape[0], pls.head()))

        rmlst = pandas.read_csv(input.rmlst, sep='\t', header=0, dtype=str)
        logger.info('Read in {} rMLST hits\n{}'.format(rmlst.shape[0], rmlst.head()))

        # count unique loci per query
        pls['hits_rMLST'] = ''
        pls['hitscount_rMLST'] = 0
        for qseqid, df in rmlst[['qseqid', 'slocus']].groupby(['qseqid']):
            hits = sorted(list(set(df['slocus'])))
            pls.loc[qseqid, 'hits_rMLST'] = ';'.join(hits)
            pls.loc[qseqid, 'hitscount_rMLST'] = len(hits)
        pls.to_csv('%s.backup' % output[0], sep='\t', header=True, index=False, index_label=False)

        # check those above given cutoff
        check_ids = pls.loc[pls['hitscount_rMLST'] > params.cutoff,'ACC_NUCCORE']
        # run BLASTn
        from utils import run_blastn_check
        pool = Pool(params.cores)
        tmp = pool.starmap(
            run_blastn_check,
            [(acc, output[0], input.fna, config['rmlst']['blastn'], params.blastn_bin, params.blastn_header, params.blastn_pident) for acc in check_ids]
        )
        pool.close()
        pool.join()
        # check results
        for acc in check_ids:
            # clean up: rm FASTA
            acc_fasta = '%s.%s.fna' % (output[0], acc)
            if os.path.exists(acc_fasta):
                os.remove(acc_fasta)
            # load hits
            acc_ofile = '%s.%s.tsv' % (output[0], acc)
            try:
                acc_df = pandas.read_csv(acc_ofile, sep='\t', header=None, names=params.blastn_header)
            except pandas.errors.EmptyDataError:
                logger.info('KEEP {}: no blastn hits'.format(acc))
                continue
            # filter hits
            acc_df = acc_df.loc[(acc_df['pident'] >= params.blastn_pident) & (acc_df['qcovs'] >= params.blastn_qcovs),:]
            # no hits left
            if acc_df.shape[0] == 0:
                logger.info('KEEP {}: no blastn hits left after filtering (pident {}, qcovs {})'.format(acc, params.blastn_pident, params.blastn_qcovs))
                continue
            else:
                top1 = acc_df.iloc[0,:]
                logger.info('RM {}: 1st hit: {} [{}] (evalue {}, pident {}, qcovs {})'.format(acc, top1['sseqid'], top1['stitle'], top1['evalue'], top1['pident'], top1['qcovs']))
                pls.drop(index=[acc], inplace=True)
        logger.info('Filtered: by number of rMLST and blastn hits: kept {} records'.format(pls.shape[0]))

        # save
        pls.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

######################################################################
# MASTER FILES
######################################################################
rule master_fasta:
    input:
        pls=PLASMIDS_FILT3,
        fna=PLASMIDS_FILT2_FASTA
    output:
        MASTER_FASTA
    message:
        "Create FASTA file for records in {input.pls}"
    run:
        from Bio import SeqIO

        logger = setup_logger(logging.INFO)

        mkdir(os.path.dirname(output[0]), True)

        ids = list(pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)['ACC_NUCCORE'])
        n   = len(ids)
        ids = set(ids)
        assert len(ids) == n, 'FASTA IDs in {} are not unique'.format(input.pls)
        logger.info('Read in {} plasmid records'.format(len(ids)))

        with open(input.fna, 'r') as ifile, open(output[0], 'w') as ofile:
            for record in tqdm(SeqIO.parse(ifile, 'fasta')):
                if record.id in ids:
                    SeqIO.write(record, ofile, 'fasta')
                    ids.remove(record.id)
        assert len(ids) == 0, 'Not all IDs found in FASTA {}: {}'.format(input.fna, ';'.join(ids))

# ABRicate annot
##################################################
# NOTE: Using a procedure analoguous to that of PlasmidFinder (https://bitbucket.org/genomicepidemiology/plasmidfinder)
#   1) Call Blaster (from CGE core module): call BLAST and pre-process hits (best hit per subject)
#   2) Filter results by identity and coverage; from overlapping hits keep only the best hit
#   Differences/modifications: see tag "CHANGED"
rule abricate:
    input:
        fna="{basename}.fna",
        abr=ABRICATE_UPDATE,
        blastn=BIN_BLASTN
    output:
        "{basename}.abr.{rdb}"
    message:
        "Search {wildcards.rdb} in fasta: {input.fna}"
    params:
        cov=lambda wildcards: config['abricate']['params'][wildcards.rdb]['cov'],
        ident=lambda wildcards: config['abricate']['params'][wildcards.rdb]['ident']
    run:
        from utils import call_blaster
        logger = setup_logger(logging.INFO)

        db_path = os.path.join(ENV_PATH, 'db', wildcards.rdb)
        logger.info('Blaster database in {}'.format(db_path))

        # call Blaster to get and pre-process BLAST hits
        df_all = []
        df = []
        for record in tqdm(SeqIO.parse(input.fna, 'fasta')):
            # call Blaster: get BLAST hits and pre-process them
            result = call_blaster(record=record, fasta=input.fna, db_path=db_path, blast=input.blastn, cov=params.cov, ident=params.ident)
            # no results
            if not result:
                continue

            # convert to table
            result = pandas.DataFrame(result)

            # filter by coverage and identity
            result = result.loc[(result['cov'] >= params.cov) & (result['pident'] >= params.ident),:]
            # no results left
            if result.shape[0] == 0:
                continue

            # compute score from identity and coverage
            result = result.assign(ic_score = result['pident'] * result['cov'])

            # sort by query start: small -> large (ascending)
            result.sort_values(by=['qstart'], ascending=[True], inplace=True)
            df_all.append(result)

            # overlapping hits
            keep_hit = [True] * result.shape[0] # True = keep, False = remove
            current_i   = 0
            current_end = result['qend'].values[0]
            current_ics = result['ic_score'].values[0]
            for i in range(0, result.shape[0]-1):
                next_start = result['qstart'].values[i+1]
                next_end   = result['qend'].values[i+1]
                next_ics   = result['ic_score'].values[i+1]

                if next_start <= current_end: # overlapping hits; CHANGED: <=, NOT <
                    # remove hit with lower identity-coverage score
                    if current_ics < next_ics:
                        keep_hit[current_i] = False
                        # update current
                        current_i   = i+1
                        current_end = next_end
                        current_ics = next_ics
                    else:
                        keep_hit[i+1] = False
                else: # no overlap, update current; CHANGED: otherwise if no overlap always comparing to the latest "current"
                    current_i   = i+1
                    current_end = next_end
                    current_ics = next_ics
            result = result.loc[keep_hit,:]

            # rm coverage-identity score
            result = result.drop(['ic_score'], axis=1, inplace=False) # inplace=True produces SettingWithCopyWarning

            # save
            df.append(result)
        df_all = pandas.concat(df_all, axis=0, sort=False)
        df = pandas.concat(df, axis=0, sort=False)

        # stitle -> DB + ID
        # title format: <BLAST DB ID> <DB>~~~<name>~~~<ID> <name>
        # example: gnl|BL_ORD_ID|578 argannot~~~(Bla)blaFRI-3~~~KY524440:1-885 (Bla)blaFRI-3
        df_all['sseqdb'] = df_all['stitle'].map(lambda x: x.split(' ')[1].split('~~~')[0])
        df_all['sseqid'] = df_all['stitle'].map(lambda x: ', '.join(x.split(' ')[1].split('~~~')[1:]))
        df['sseqdb']     = df['stitle'].map(lambda x: x.split(' ')[1].split('~~~')[0])
        df['sseqid']     = df['stitle'].map(lambda x: ', '.join(x.split(' ')[1].split('~~~')[1:]))

        # save
        df_all.to_csv(
            path_or_buf=output[0]+'.all',
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )
        df.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )

rule cat_abricate:
    input:
        ["{basename}.abr.%s" % db for db in config['abricate']['dbs']]
    output:
        "{basename}.abr"
    message:
        "Concat ABRicate hits: {input}"
    params:
        blastn=BIN_BLASTN,
        cores=10
    run:
        dfs = []
        for ifile in input:
            dfs.append(pandas.read_csv(ifile, sep='\t', header=0))
        dfs = pandas.concat(dfs, axis=0, sort=False)
        dfs.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )

# pMLST
#################################################
rule pmlst:
    input:
        db=PMLST_DB,
        fasta="{path}/{date}.fna",
        pf="{path}/{date}.abr"
    output:
        "{path}/{date}.pmlst"
    message:
        "pmlst on fasta: {input}"
    run:
        import shutil
        from Bio import SeqIO
        from utils import process_pmlst_hits

        logger = setup_logger(logging.INFO)

        # PlasmidFinder
        pf = pandas.read_csv(input.pf, sep='\t', header=0)
        pf = pf.loc[pf['sseqdb'] == 'plasmidfinder',]
        logger.info('Read in {} PlasmidFinder hits\n{}'.format(pf.shape[0], pf.head()))

        # pMLST
        pmlst_hits = []
        tmp_file = 'pmlst.tmp'
        tmp_fasta = os.path.join(tmp_file + '.fasta')
        logger.info('Run pMLST for each record with available scheme w.r.t. PlasmidFinder hits.')
        with open(input.fasta, 'r') as ifile:
            for record in tqdm(SeqIO.parse(ifile, 'fasta')):
                # found replicons
                record_replicons = list(pf.loc[pf['qseqid'] == record.id,'sseqid'])
                if len(record_replicons) == 0:
                    continue
                else:
                    SeqIO.write(record, open(tmp_fasta, 'w'), 'fasta') # create tmp FASTA

                used_schemes = set()
                for record_replicon in record_replicons:
                    # set scheme
                    scheme = None
                    for k,v in config['pmlst']['map'].items():
                        if re.match(k, record_replicon, re.IGNORECASE):
                            scheme = v
                            break
                    if scheme is None or scheme in used_schemes: # no matching scheme/already done
                        continue
                    else:
                        used_schemes.add(scheme)

                    # run pmlst
                    cmd = config['pmlst']['cmd'].format(scheme=scheme) + " {fasta} > {ofile}".format(fasta=tmp_fasta, ofile=tmp_file)
                    cmd, cmd_s, cmd_o = run_cmd(cmd)
                    assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)

                    # process hits
                    hits = process_pmlst_hits(f=tmp_file, pmlst_db_path=os.path.join(ENV_PATH, 'db/pubmlst'), logger=logger)
                    for i in range(0, len(hits)):
                        hits[i] = {
                            'ID': record.id,
                            'pmlst': hits[i]
                        }
                    pmlst_hits += hits

        # add pMLST hits to PlasmidFinder hits
        pmlst_hits = pandas.DataFrame(pmlst_hits)
        # pmlst_hits.set_index(keys='ID', drop=True, inplace=True, verify_integrity=True)
        # pfs = pandas.merge(
        #     left=pfs,
        #     right=pmlst_hits,
        #     how='left',
        #     left_index=True,
        #     right_index=True,
        #     sort=False,
        # )

        # save
        pmlst_hits.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )

        # clean up
        os.remove(tmp_file)
        os.remove(tmp_fasta)

# BLASTn DBs
##################################################
# See "general" rules

# Mash
##################################################
# See "general" rules

# Similar records
##################################################
rule sim_records:
    input:
        MASTER_MASH_DISTS
    output:
        MASTER_SIM
    message:
        'Lits of similar records pairs from {input}'
    run:
        pairs = set()
        with open(input[0], 'r') as ifile, open(output[0], 'w') as ofile:
            for line in tqdm(ifile):
                sID, qID, dist, pv, sh = line.rstrip('\n').split('\t')
                pair = sorted([sID, qID])
                pair = (pair[0], pair[1])
                # same ID -> skip
                if sID == qID:
                    continue
                # group ID already set -> skip
                if pair in pairs:
                    continue
                else:
                    pairs.add(pair)
                    ofile.write('{}\t{}\n'.format(sID, qID))

# Embedding
##################################################
# See "general" rules

# Info table
##################################################
rule infotable:
    input:
        pls=PLASMIDS_FILT3,
        emb="%s/{date}.umap" % config["data"]["odir"]["master"],
        abr="%s/{date}.abr" % config["data"]["odir"]["master"],
        pmlst="%s/{date}.pmlst" % config["data"]["odir"]["master"]
    output:
        "%s/{date}.tsv" % config["data"]["odir"]["master"]
    message:
        "Create info table for records in {input.pls}"
    run:
        logger = setup_logger(logging.INFO)

        # plasmids
        pls = pandas.read_csv(input.pls, sep='\t', header=0, dtype=str)
        pls.set_index(keys='ACC_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        logger.info('Read in {} plasmid records\n{}'.format(pls.shape[0], pls.head()))

        # embedding
        emb = pandas.read_csv(input.emb, sep='\t', header=0, index_col='ID', dtype=str)
        logger.info('Read in embedding for {} records\n{}'.format(emb.shape[0], emb.head()))

        # PlasmidFinder
        abr = pandas.read_csv(input.abr, sep='\t', header=0)
        abr = abr.loc[abr['sseqdb'] == 'plasmidfinder',['qseqid', 'sseqid']]
        logger.info('Read in {} PlasmidFinder hits\n{}'.format(abr.shape[0], abr.head()))
        pf = []; pf_index = []
        for aggr_id, aggr_df in abr.groupby(['qseqid']):
            assert len(set(aggr_df['sseqid'])) == len(aggr_df['sseqid'])
            pf_index.append(list(aggr_df['qseqid'])[0])
            pf.append({'plasmidfinder': '|'.join(aggr_df['sseqid'])})
        pf = pandas.DataFrame(pf, index=pf_index)
        logger.info('Aggr PlasmidFinder hits\n{}'.format(pf.head()))

        # pMLST
        pmlst_df = pandas.read_csv(input.pmlst, sep='\t', header=0)
        logger.info('Read in {} pMLST records\n{}'.format(pmlst_df.shape[0], pmlst_df.head()))
        pmlst = []; pmlst_index = []
        for aggr_id, aggr_df in pmlst_df.groupby(['ID']):
            assert len(set(aggr_df['pmlst'])) == len(aggr_df['pmlst'])
            pmlst_index.append(list(aggr_df['ID'])[0])
            pmlst.append({'pmlst': '|'.join(aggr_df['pmlst'])})
        pmlst = pandas.DataFrame(pmlst, index=pmlst_index)
        logger.info('Aggr pMLST hits\n{}'.format(pmlst.head()))

        # process source strings
        source_map = {
            'insd': 'INSDC',
            'refseq': 'RefSeq'
        }
        pls['Source_NUCCORE'] = pls['Source_NUCCORE'].map(lambda x: source_map[x])

        # add data
        pls = pandas.merge(
            left=pls,
            right=emb,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        logger.info('Added embedding')

        pls = pandas.merge(
            left=pls,
            right=pf,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        logger.info('Added PlasmidFinder hits')

        pls = pandas.merge(
            left=pls,
            right=pmlst,
            how='left',
            left_index=True,
            right_index=True,
            sort=False,
        )
        logger.info('Added pMLST hits')

        # save
        pls.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )

# Krona plot
##################################################
rule krona_xml:
    input:
        MASTER_TAB
    output:
        MASTER_KRONA_XML
    message:
        "Create XML file for Krona plot from {input}"
    shell:
        "python create_krona_xml.py -t {input} -o {output}"

rule krona_html:
    input:
        MASTER_KRONA_XML
    output:
        MASTER_KRONA_HTML
    message:
        "Create Krona plot from {input}"
    params:
        bin=BIN_KRONA_XML
    shell:
        "{params.bin} {input} -o {output}"

# Summary
##################################################
rule summary:
    input:
        tab=MASTER_TAB,
        script='summary.R'
    output:
        pdf=MASTER_SUM_PLOTS,
        txt=MASTER_SUM_STATS
    message:
        "Summary of collected data: plots and statistics"
    params:
        width=10,
        height=6
    shell:
        "Rscript {input.script} --tab {input.tab} --pdf {output.pdf} --width {params.width} --height {params.height} | tee {output.txt}"

# Compare to older version
##################################################
rule compare:
    input:
        new=MASTER_TAB,
        old=config['old_tab'],
        new_nonfiltered=PLASMIDS_FULL2,
        script='compare_master_tabs.R'
    output:
        MASTER_CHANGES
    shell:
        "Rscript {input.script} -n {input.new} -o {input.old} -f {input.new_nonfiltered} -t {output} -l {output}.log"

######################################################################
# GENERAL RULES
######################################################################

# FASTA for nuccore UIDs in table
rule get_fasta:
    input:
        "{basename}.txt"
    output:
        "{basename}.fna"
    message:
        "Downloading sequences for IDs in {input}"
    params:
        eutils=config['eutils']['dir']
    shell:
        """
        mkdir -p $(dirname {output}) &&
        python download_fastas.py -t {input} -i \"UID_NUCCORE\" -o {output} -c 1 -s 500 -e {params.eutils} 2>&1 | tee {output}.log &&
        cat {output}.tmp.* > {output} && rm {output}.tmp.*
        """

# BLAST DBs from FASTA
rule blastndb:
    input:
        fasta="{basename}.fna",
        bin=BIN_MAKEBLASTDB
    output:
        ["{basename}.fna.%s" % ext for ext in config['blast']['ext']]
    log:
        "{basename}.makeblastdb.log"
    params:
        date=today
    message:
        "Create BLASTDB from {input.fasta}"
    shell:
        "{input.bin} -in {input.fasta} -input_type fasta -dbtype nucl -title \"plsdb_{params.date}\" -logfile {log}"

# Mash sketch
rule mash_sketch:
    input:
        fasta="{basename}.fna",
        bin=BIN_MASH
    output:
        "{basename}.msh"
    params:
        params=config['mash']['sketch_params']
    message:
        "Create Mash signatures from {input}"
    shell:
        "{input.bin} sketch {params.params} -o $(dirname {output})/$(basename -s .msh {output}) {input.fasta}"

# Mash dist (only if dist = 0)
rule mash_dist_zero:
    input:
        "{basename}.msh"
    output:
        "{basename}.dist0"
    params:
        params=config['mash']['dist0_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input} {input} > {output}"

# Mash dist for highly sim. seq.s (dist cutoff)
rule mash_dist_sim:
    input:
        "{basename}.msh"
    output:
        "{basename}.distS"
    params:
        params=config['mash']['distS_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input} {input} > {output}"

# Mash dist (all, tab format)
rule mash_dist:
    input:
        "{basename}.msh"
    output:
        "{basename}.dist"
    params:
        params=config['mash']['dist_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input} {input} > {output}"

# Embedding using UMAP on Mash distances
rule umap:
    input:
        "{basename}.dist"
    output:
        "{basename}.umap"
    log:
        "{basename}.umap.log"
    params:
        neighbors=config['umap']['neighbors'],
        components=config['umap']['components'],
        min_dist=config['umap']['min_dist']
    message:
        "UMAP on {input}"
    run:
        import umap, numpy

        logger = setup_logger(logging.INFO)

        # IDs
        ids = None
        with open(input[0], 'r') as ifile:
            ids = ifile.readline().rstrip('\n').split('\t')[1:]
            logger.info('Distance matrix: {num} x {num}'.format(num=len(ids)))

        # distance matrix
        logger.info('Start loading distances...')
        dist = numpy.loadtxt(
            fname=input[0],
            skiprows=1,
            usecols=range(1,len(ids) + 1) # skip 1st (contains IDs)
        )

        # embedding
        logger.info('Start embedding...')
        embedding = umap.UMAP(
            n_neighbors=params.neighbors,
            n_components=params.components,
            min_dist=params.min_dist,
            init='random',
            metric='precomputed',
            random_state=42
        ).fit_transform(dist)
        logger.info('Done.')

        # save to file
        embedding = pandas.DataFrame(
            embedding,
            columns=['D1', 'D2'],
            index=ids
        )
        embedding.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=True,
            index_label='ID'
        )
