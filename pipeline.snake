##################################################
# Config
##################################################
CONFIG_JSON = "pipeline.json"
configfile: CONFIG_JSON

##################################################
# Python modules
##################################################
import os
import re
import pandas
import pickle
import logging
import datetime
from tqdm import tqdm
from Bio import SeqIO
from collections import OrderedDict

from utils import preproc_loc_str, preproc_loc_coords, parse_location, setup_logger

##################################################
# Some settings
##################################################

# Date
# today = datetime.datetime.now().strftime("%Y_%m_%d")
today = "2018_07_25"

# BLAST DB file ext.
blastdb_ext = ['nin', 'nhr', 'nsq']

# NCBI DBs
dbs = OrderedDict()
dbs['genbank'] = 'GenBank'
dbs['refseq'] = 'RefSeq'

##################################################
# Tools
##################################################
# BLAST
BIN_MAKEBLASTDB = os.path.join(config["blast"]["dir"], "bin/makeblastdb")

# Mash
BIN_MASH = os.path.join(config["mash"]["dir"], "mash")

# edirect/eutils
EUTILS_INSTALL = os.path.join(config["eutils"]["dir"], "install.done")

# Krona
BIN_KRONA_XML = os.path.join(config["krona"]["dir"], "bin/ktImportXML")

##################################################
# Data
##################################################
# Table with plasmid records
PLASMIDS_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__nuccore.txt' % (today, db)) for db in dbs.keys()]

# Associated BioSamples
PLASMIDS_BIOS_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__linkbiosamples.txt' % (today, db)) for db in dbs.keys()]

# Table with BioSamples
BIOSAMPLES_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__biosamples.txt' % (today, db)) for db in dbs.keys()]
BIOSAMPLES_LOCS_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__biosamples_locs.txt' % (today, db)) for db in dbs.keys()]

# Table with taxa
TAXA_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__taxa.txt' % (today, db)) for db in dbs.keys()]
TAXA_PROC_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__taxaproc.txt' % (today, db)) for db in dbs.keys()]

##################################################
# Data: "Master" files
##################################################
# Plasmid sequences
MASTER_GFASTA = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.fna" % today), db=dbs.keys())

# BLASTdb from master FASTA
MASTER_BLASTDB = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.fna.{ext}" % today), db=dbs.keys(), ext=blastdb_ext)

# Mash
MASTER_MASH_SIG  = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.msh" % today), db=dbs.keys())
MASTER_MASH_DIST = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.dist" % today), db=dbs.keys())

# UMAP
MASTER_MASH_UMAP = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.umap" % today), db=dbs.keys())

# Master info table
MASTER_TAB = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.tsv" % today), db=dbs.keys())

# Krona plot
MASTER_KRONA_XML = os.path.join(config["data"]["odir"]["master"], "%s.xml" % today)
MASTER_KRONA_HTML = os.path.join(config["data"]["odir"]["master"], "%s.html" % today)

##################################################
# Rules
##################################################

# ALL rule
##################################################
rule all:
	input:
        # tools
		blast=BIN_MAKEBLASTDB,
        mash=BIN_MASH,
        eutils=EUTILS_INSTALL,
        krona=BIN_KRONA_XML,
        # plasmids
        query_plasmids=PLASMIDS_TAB,
        # linked samples
        query_bios=PLASMIDS_BIOS_TAB,
        query_biosamples=[BIOSAMPLES_TAB, BIOSAMPLES_LOCS_TAB],
        # linked taxa
        query_taxa=[TAXA_TAB, TAXA_PROC_TAB],
		# master files
        # fasta
        master_fasta=MASTER_GFASTA,
        # blastdb
		master_blastdb=MASTER_BLASTDB,
		# mash
        master_sig=MASTER_MASH_SIG,
        master_dist=MASTER_MASH_DIST,
		# umap
        master_umap=MASTER_MASH_UMAP,
        # table
        master_tab=MASTER_TAB,
        # krona plot
        master_krona=[MASTER_KRONA_XML, MASTER_KRONA_HTML]

# Tools
##################################################
rule install_blast:
    output:
        BIN_MAKEBLASTDB
    params:
        url=config["blast"]["url"],
        name=os.path.basename(config["blast"]["url"])
    message:
        "installing blast from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xzvf {params.name} && rm {params.name}"

rule install_mash:
    output:
        BIN_MASH
    params:
        url=config["mash"]["url"],
        name=os.path.basename(config["mash"]["url"])
    message:
        "installing mash from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xvf {params.name} && rm {params.name}"

rule install_eutils:
    output:
        EUTILS_INSTALL
    message:
        "Install edirect/euils"
    shell:
        """
        mkdir -p $(dirname {output}) && cd tools
        perl -MNet::FTP -e \
            '$ftp = new Net::FTP("ftp.ncbi.nlm.nih.gov", Passive => 1);
             $ftp->login; $ftp->binary;
             $ftp->get("/entrez/entrezdirect/edirect.tar.gz");'
        gunzip -c edirect.tar.gz | tar xf -
        rm edirect.tar.gz
        cd edirect && wget https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/xtract.Linux.gz && gunzip -f xtract.Linux.gz && chmod +x xtract.Linux
        touch $(basename {output})
        """

rule install_krona:
    output:
        BIN_KRONA_XML
    message:
        "Install Krona"
    params:
        url=config["krona"]["url"],
        name=os.path.basename(config["krona"]["url"])
    shell:
        """
        mkdir -p tools && cd tools && wget {params.url} && unzip {params.name} && rm {params.name} &&
        rsync -av Krona-xl2.5/KronaTools . && rm -r Krona-xl2.5/ && cd KronaTools && ./install.pl --prefix .
        """

# Make queries & filtering
##################################################
rule query_plasmids:
    output:
        "{dir}/{date}__{db}__nuccore.txt"
    message:
        "Query for plasmids in {wildcards.db}"
    params:
        query=lambda wildcards: config['eutils']['query']['plasmid'].format(db=wildcards.db, path=config['eutils']['dir']),
        header='\t'.join(config['eutils']['header']['plasmid'])
    shell:
        """
        mkdir -p $(dirname {output}) &&
        echo -e \"{params.header}\" > {output}
        {params.query} >> {output}
        """

# BioSamples
##################################################
rule query_linked_biosamples:
    input:
        "{dir}/{date}__{db}__nuccore.txt"
    output:
        "{dir}/{date}__{db}__linkbiosamples.txt"
    message:
        "Query for linked BioSamples from {input}"
    params:
        query_before=config['eutils']['query']['link_bios']['before'].format(path=config['eutils']['dir']),
        query_after=config['eutils']['query']['link_bios']['after'].format(path=config['eutils']['dir']),
        header='\t'.join(config['eutils']['header']['link_bios'])
    run:
        from utils import split_list, run_cmd
        setup_logger(logging.INFO)
        ids = pandas.read_csv(input[0], sep='\t', header=0)['UID_NUCCORE'].values
        assert len(set(ids)) == len(ids)
        logging.info('There are {} unique IDs'.format(len(ids)))
        with open(output[0], 'w') as ofile:
            ofile.write(params.header + '\n')
            for chunk in tqdm(split_list(ids, 200)):
                cmd = '{} \"{}\" {}'.format(params.query_before, ','.join([str(i) for i in chunk]), params.query_after)
                cmd, cmd_s, cmd_o = run_cmd(cmd)
                assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)
                ofile.write(cmd_o)
        # checks
        result = pandas.read_csv(output[0], sep='\t', header=0, index_col=0)
        for i in result.index:
            i_b = result.loc[i, 'UID_BIOSAMPLE']
            assert i in ids, 'Unexpected nuccore ID {}'.format(i)
            assert pandas.isnull(i_b) or (not re.search(';', str(i_b))), 'Multiple hits for {}: {}'.format(i, i_b)

rule query_biosamples:
    input:
        "{dir}/{date}__{db}__linkbiosamples.txt"
    output:
        "{dir}/{date}__{db}__biosamples.txt"
    message:
        "Query for biosamples from {input}"
    params:
        query_before=config['eutils']['query']['biosample']['before'].format(path=config['eutils']['dir']),
        query_after=config['eutils']['query']['biosample']['after'].format(path=config['eutils']['dir']),
        header='\t'.join(config['eutils']['header']['biosample'])
    run:
        from utils import split_list, run_cmd
        setup_logger(logging.INFO)
        ids = pandas.read_csv(input[0], sep='\t', header=0)['UID_BIOSAMPLE']
        ids = set(ids.loc[pandas.notnull(ids)].values)
        logging.info('There are {} unique IDs'.format(len(ids)))
        with open(output[0], 'w') as ofile:
            ofile.write(params.header + '\n')
            for chunk in tqdm(split_list(ids, 200)):
                cmd = '{} \"{}\" {}'.format(params.query_before, ','.join([str(i) for i in chunk]), params.query_after)
                cmd, cmd_s, cmd_o = run_cmd(cmd)
                assert cmd_s == 0, 'CMD: {}: {}\n{}'.format(cmd, cmd_s, cmd_o)
                ofile.write(cmd_o)
        # checks
        result = pandas.read_csv(output[0], sep='\t', header=0)
        not_found = list(ids.difference(result['UID_BIOSAMPLE'].values)) # not found IDs
        une_found = list(set(result['UID_BIOSAMPLE']).difference(ids)) # unexpected IDs, i.e. should not be searched
        assert result.shape[0] == len(set(result['UID_BIOSAMPLE']))
        assert len(not_found) == 0, 'Not found: {}: {}, ...'.format(len(not_found), ', '.join(not_found[0:5]))
        assert len(une_found) == 0, 'Not expected: {}: {}, ...'.format(len(une_found), ', '.join(une_found[0:5]))

# Taxa
##################################################
rule query_taxon:
    input:
        "{dir}/{date}__{db}__nuccore.txt"
    output:
        "{dir}/{date}__{db}__taxa.txt"
    message:
        "Query for taxa from {input}"
    params:
        query_before=config['eutils']['query']['taxon']['before'].format(path=config['eutils']['dir']),
        query_after=config['eutils']['query']['taxon']['after'].format(path=config['eutils']['dir']),
        col=config['eutils']['header']['plasmid'].index('TaxonID_NUCCORE') + 1
    shell:
        """
        {params.query_before} \"$(tail -n +2 {input} | cut -f{params.col} | grep -v '^NA$' | sort | uniq | tr \'\n\' \' \')\" {params.query_after} > {output}
        """

rule proc_taxon:
    input:
        "{dir}/{date}__{db}__taxa.txt"
    output:
        "{dir}/{date}__{db}__taxaproc.txt"
    message:
        "Process taxa from {input}"
    run:
        header  = ["taxon_id", "taxon_name", "taxon_rank", "lineage"]
        for rank in config['eutils']['header']['ranks']:
            header += ['taxon_%s_id' % rank, 'taxon_%s_name' % rank]
        with open(output[0], 'w') as ofile:
            ofile.write('\t'.join(header) + '\n')
            with open(input[0], 'r') as ifile:
                for line in ifile:
                    info = dict.fromkeys(header,'NA')
                    line = line.rstrip('\n')
                    for field in line.split('\t'):
                        if re.search('\|',field):
                            tid, tname, trank = field.split('|')
                            if trank == 'no rank':
                                trank = 'NA'
                            elif "taxon_%s_name" % trank in header:
                                info["taxon_%s_name" % trank] = tname
                                info["taxon_%s_id" % trank] = tid
                            if info['taxon_id'] == 'NA':
                                info['taxon_id'] = tid
                                info['taxon_name'] = tname
                                info['taxon_rank'] = trank
                        else: # lineage
                            info['lineage'] = field
                    assert info['taxon_id'] != "NA" and info['taxon_name'] != "NA"
                    ofile.write('\t'.join([info[k] for k in header]) + '\n')

# Locations
##################################################
rule parse_locations:
    input:
        tab="{dir}/{date}__{db}__biosamples.txt"
    output:
        "{dir}/{date}__{db}__biosamples_locs.txt"
    message:
        "Parsing locations from {input}"
    params:
        api_keys=config['data']['gmaps_api_keys'],
        pck=config['data']['locs']
    run:
        # api keys
        with open(params.api_keys, 'r') as ifile:
            api_key = [k.rstrip('\n').strip() for k in ifile.readlines()]
            api_key = api_key.pop()
        logging.info('Using API key: %s' % api_key)
        # load known locations
        locs = dict()
        if os.path.exists(params.pck):
            with open(params.pck, 'rb') as ifile:
                locs = pickle.load(ifile)
        # sample table with locations
        df = pandas.read_csv(input.tab, header=0, index_col=0, sep='\t')
        df['loc_lat'] = None
        df['loc_lng'] = None
        # parse
        for i in df.index:
            # location name and coordinates
            l_n = df.loc[i,'Location_BIOSAMPLE']
            l_c = df.loc[i,'Coordinates_BIOSAMPLE']

            # pre-processing
            l_n = preproc_loc_str(l_n)
            l_c = preproc_loc_coords(preproc_loc_str(l_c))

            # no location data
            if l_n is None and l_c is None:
                continue

            # at least name or coordinates given
            if l_c is not None:
                if l_c not in locs:
                    logging.info('Retrieving coordinates for location: \"{}\"'.format(l_c))
                    try:
                        locs[l_c] = parse_location(loc_str=l_c, api_key=api_key, is_name=False)
                        with open(params.pck, "wb") as ofile:
                            pickle.dump(locs, ofile)
                    except Exception as e:
                        logging.error('Error while retrieving coordinates for location \"{}\"'.format(l_c))
                        raise(e)
                df.loc[i,'loc_lat'] = locs[l_c]['lat']
                df.loc[i,'loc_lng'] = locs[l_c]['lng']
            elif l_n is not None:
                if l_n not in locs:
                    logging.info('Retrieving coordinates for location: \"{}\"'.format(l_n))
                    try:
                        locs[l_n] = parse_location(loc_str=l_n, api_key=api_key, is_name=True)
                        with open(params.pck, "wb") as ofile:
                            pickle.dump(locs, ofile)
                    except Exception as e:
                        logging.error('Error while retrieving coordinates for location \"{}\"'.format(l_n))
                        raise(e)
                df.loc[i,'loc_lat'] = locs[l_n]['lat']
                df.loc[i,'loc_lng'] = locs[l_n]['lng']
        df.to_csv(output[0], sep='\t', header=True, index=True, index_label='UID_BIOSAMPLE')

# FASTAs
##################################################
# Download FASTAs
rule download_seqs:
    input:
        "%s/{date}__{db}__nuccore.txt" % config['data']['odir']['reports']
    output:
        "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    message:
        "Downloading sequences for IDs in {input}"
    params:
        eutils=config['eutils']['dir']
    shell:
        """
        mkdir -p $(dirname {output}) &&
        python download_fastas.py -t {input} -i \"UID_NUCCORE\" -o {output} -c 20 -s 500 -e {params.eutils} 2>&1 | tee {output}.log &&
        cat {output}.tmp.* > {output} && rm {output}.tmp.*
        """

# BLASTn DBs
##################################################
rule blastndb:
    input:
        "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    output:
        ["%s/{date}__{db}.fna.%s" % (config["data"]["odir"]["master"], ext) for ext in blastdb_ext]
    log:
        "%s/{date}__{db}.makeblastdb.log" % config["data"]["odir"]["master"]
    params:
        bin=BIN_MAKEBLASTDB
    message:
        "Create BLASTDB from {input}"
    shell:
        "{params.bin} -in {input} -input_type fasta -dbtype nucl -title \"ncbi__{wildcards.db}__{wildcards.date}\" -logfile {log}"

# Mash
##################################################
# signatures
rule mash_sig:
    input:
        "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.msh" % config["data"]["odir"]["master"]
    log:
        "%s/{date}__{db}.msh.log" % config["data"]["odir"]["master"]
    params:
        params=config['mash']['sketch_params'],
        mash=BIN_MASH
    message:
        "Create signatures using Mash on {input}"
    shell:
        "{params.mash} sketch {params.params} -o $(dirname {output})/$(basename -s .msh {output}) {input} 2>&1 | tee {log}"

# distance matrix
rule mash_dist:
    input:
        "%s/{date}__{db}.msh" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.dist" % config["data"]["odir"]["master"]
    log:
        "%s/{date}__{db}.dist.log" % config["data"]["odir"]["master"]
    params:
        params=config['mash']['dist_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input} {input} > {output} 2> {log}"

# Embedding
##################################################
rule umap:
    input:
        "%s/{date}__{db}.dist" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.umap" % config["data"]["odir"]["master"]
    log:
        "%s/{date}__{db}.umap.log" % config["data"]["odir"]["master"]
    params:
        neighbors=config['umap']['neighbors'],
        components=config['umap']['components']
    message:
        "UMAP on {input}"
    run:
        import umap
        import numpy
        # IDs
        ids = None
        with open(input[0], 'r') as ifile:
            ids = ifile.readline().rstrip('\n').split('\t')[1:]
            logging.info('Distance matrix: {num} x {num}'.format(num=len(ids)))
        # distance matrix
        logging.info('Start loading distances...')
        dist = numpy.loadtxt(
            fname=input[0],
            skiprows=1,
            usecols=range(1,len(ids) + 1) # skip 1st (contains IDs)
        )
        logging.info('Done.')
        # embedding
        logging.info('Start embedding...')
        embedding = umap.UMAP(
            n_neighbors=params.neighbors,
            n_components=params.components,
            init='random',
            metric='precomputed'
        ).fit_transform(dist)
        logging.info('Done.')
        # save to file
        embedding = pandas.DataFrame(
            embedding,
            columns=['D1', 'D2'],
            index=ids
        )
        embedding.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=True,
            index_label='ID'
        )

# Info table
##################################################
rule infotable:
    input:
        plasmids="%s/{date}__{db}__nuccore.txt" % config["data"]["odir"]["reports"],
        linked_samples="%s/{date}__{db}__linkbiosamples.txt" % config["data"]["odir"]["reports"],
        samples="%s/{date}__{db}__biosamples_locs.txt" % config["data"]["odir"]["reports"],
        taxa="%s/{date}__{db}__taxaproc.txt" % config["data"]["odir"]["reports"],
        embedding="%s/{date}__{db}.umap" % config["data"]["odir"]["master"],
        fasta="%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.tsv" % config["data"]["odir"]["master"]
    message:
        "Create info table for records in {input.plasmids}"
    run:
        from Bio import SeqIO
        from Bio.SeqUtils import GC

        # read in data
        # plasmids
        pls = pandas.read_csv(input.plasmids, sep='\t', header=0, index_col=None, dtype=str)
        pls[['Length_NUCCORE']] = pls[['Length_NUCCORE']].apply(pandas.to_numeric)
        pls.set_index(keys='ACC_NUCCORE', drop=False, inplace=True, verify_integrity=True)
        # linked samples
        lsmp = pandas.read_csv(input.linked_samples, sep='\t', header=None, names=['UID_NUCCORE', 'UID_BIOSAMPLE'], dtype=str)
        lsmp.set_index(keys='UID_NUCCORE', drop=True, inplace=True, verify_integrity=True)
        # samples
        smp = pandas.read_csv(input.samples, sep='\t', header=0, index_col=None, dtype=str)
        smp.set_index(keys='UID_BIOSAMPLE', drop=True, inplace=True, verify_integrity=True)
        # taxonomy
        tax = pandas.read_csv(input.taxa, sep='\t', header=0, index_col=None, dtype=str)
        # embedding
        emb = pandas.read_csv(input.embedding, sep='\t', header=0, index_col='ID', dtype=str)

        # add info from FASTA
        logging.info('Adding info from FASTA {}'.format(input.fasta))
        pls['ACC_FASTA'] = None
        pls['GC_NUCCORE'] = None
        with open(input.fasta, 'r') as ifile:
            for record in tqdm(SeqIO.parse(ifile, 'fasta')):
                assert re.fullmatch(r'\w+\.\d+', record.id), 'Unexpected ID format in FASTA: {}'.format(record.id)
                rid = record.id.split('.')[0]
                assert rid in pls.index.values, 'FASTA record ID {}: not in plasmid table'.format(rid)
                assert pls.loc[rid,'GC_NUCCORE'] is None, 'FASTA record ID {}: GC value already set'.format(rid)
                assert len(record.seq) == pls.loc[rid,'Length_NUCCORE'], 'FASTA record ID {}: {} vs {} as length'.format(rid, len(record.seq), pls.loc[rid,'Length_NUCCORE'])
                pls.loc[rid,'ACC_FASTA'] = record.id
                pls.loc[rid,'GC_NUCCORE'] = GC(str(record.seq))

        # add info from other tables
        logging.info('Adding info from {}'.format(input.linked_samples))
        pls = pandas.merge(
            left=pls,
            right=lsmp,
            how='left',
            left_on='UID_NUCCORE',
            right_index=True,
            sort=False,
            # suffixes=('_NUCCORE', '_BIOSAMPLE'),
        )
        logging.info('Adding info from {}'.format(input.samples))
        pls = pandas.merge(
            left=pls,
            right=smp,
            how='left',
            left_on='UID_BIOSAMPLE',
            right_index=True,
            sort=False,
            # suffixes=('_NUCCORE', '_BIOSAMPLE'),
        )
        logging.info('Adding info from {}'.format(input.taxa))
        pls = pandas.merge(
            left=pls,
            right=tax,
            how='left',
            left_on='TaxonID_NUCCORE',
            right_on='taxon_id',
            sort=False,
            # suffixes=('_NUCCORE', '_TAXON'),
        )
        pls = pandas.merge(
            left=pls,
            right=emb,
            how='left',
            left_on='ACC_FASTA',
            right_index=True,
            sort=False,
            # suffixes=('_NUCCORE', '_EMB'),
        )

        # drop not needed columns
        pls.drop(columns=['TaxonID_NUCCORE'], inplace=True)

        # save
        pls.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )

# Krona plot
##################################################
rule krona_xml:
    input:
        MASTER_TAB
    output:
        MASTER_KRONA_XML
    message:
        "Create XML file for Krona plot from {input}"
    params:
        labels=' '.join(dbs.values())
    shell:
        "python create_krona_xml.py -t {input} -o {output} -l {params.labels}"

rule krona_html:
    input:
        MASTER_KRONA_XML
    output:
        MASTER_KRONA_HTML
    message:
        "Create Krona plot from {input}"
    params:
        bin=BIN_KRONA_XML
    shell:
        "{params.bin} {input} -o {output}"
