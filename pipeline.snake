##################################################
# Config
##################################################
CONFIG_JSON = "pipeline.json"
configfile: CONFIG_JSON

##################################################
# Python modules
##################################################
import os
import re
import gzip
import pandas
import pickle
import logging
import datetime
from tqdm import tqdm
from glob import glob
from Bio import SeqIO

from utils import preproc_loc_str, preproc_loc_coords, parse_location, setup_logger

##################################################
# Some settings
##################################################
# Date
today = datetime.datetime.now().strftime("%Y_%m_%d")
# today = "2018_06_28"

# BLAST DB file ext.
blastdb_ext = ['nin', 'nhr', 'nsq']

# DBs
dbs = ['refseq', 'genbank']

##################################################
# Tools
##################################################
# BLAST
BIN_MAKEBLASTDB = os.path.join(config["blast"]["dir"], "bin/makeblastdb")

# Mash
BIN_MASH = os.path.join(config["mash"]["dir"], "mash")

# Eutils
EUTILS_INSTALL = os.path.join(config["eutils"]["dir"], "install.done")

##################################################
# Data
##################################################
# Table with plasmid records
PLASMIDS_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__query.txt' % (today, db)) for db in dbs]
PLASMIDS_TAB_FILTERED = [re.sub('query', 'filtered', fname) for fname in PLASMIDS_TAB]

# Table with BioSamples
BIOSAMPLES_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__biosamples.txt' % (today, db)) for db in dbs]
BIOSAMPLES_LOCS_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__biosamples_locs.txt' % (today, db)) for db in dbs]

# Table with taxa
TAXA_TAB = [os.path.join(config['data']['odir']['reports'], '%s__%s__taxa.txt' % (today, db)) for db in dbs]
TAXA_TAB2 = [os.path.join(config['data']['odir']['reports'], '%s__%s__taxaproc.txt' % (today, db)) for db in dbs]

##################################################
# Checks
##################################################
# Retrieved BioSamples

##################################################
# Data: "Master" files
##################################################
# Plasmid sequences
MASTER_GFASTA = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.fna" % today), db=dbs)

# BLASTdb from master FASTA
MASTER_BLASTDB = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.fna.{ext}" % today), db=dbs, ext=blastdb_ext)

# Mash
MASTER_MASH_SIG  = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.msh" % today), db=dbs)
MASTER_MASH_DIST = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.dist" % today), db=dbs)

# UMAP
MASTER_MASH_UMAP = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.umap" % today), db=dbs)

# Master info table
MASTER_TAB = expand(os.path.join(config["data"]["odir"]["master"], "%s__{db}.tsv" % today), db=dbs)

##################################################
# Rules
##################################################

# ALL rule
##################################################
rule all:
	input:
        # tools
		blast=BIN_MAKEBLASTDB,
        mash=BIN_MASH,
        eutils=EUTILS_INSTALL,
        # plasmid query
        query_plasmids=PLASMIDS_TAB,
        filter_plasmids=PLASMIDS_TAB_FILTERED,
        # biosamples
        query_biosamples=BIOSAMPLES_LOCS_TAB,
        # taxa
        query_taxa=TAXA_TAB,
        proc_taxa=TAXA_TAB2,
		# master files
        # fasta
        master_fasta=MASTER_GFASTA,
        # blastdb
		master_blastdb=MASTER_BLASTDB,
		# mash
        master_sig=MASTER_MASH_SIG,
        master_dist=MASTER_MASH_DIST,
		# umap
        master_umap=MASTER_MASH_UMAP,
        # table
        master_tab=MASTER_TAB,

# Tools
##################################################
rule install_blast:
    output:
        BIN_MAKEBLASTDB
    params:
        url=config["blast"]["url"],
        name=os.path.basename(config["blast"]["url"])
    message:
        "installing blast from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xzvf {params.name} && rm {params.name}"

rule install_mash:
    output:
        BIN_MASH
    params:
        url=config["mash"]["url"],
        name=os.path.basename(config["mash"]["url"])
    message:
        "installing mash from {params.url}"
    shell:
        "mkdir -p tools && cd tools && wget {params.url} && tar -xvf {params.name} && rm {params.name}"

rule install_eutils:
    output:
        EUTILS_INSTALL
    message:
        "Install eUtils"
    shell:
        """
        mkdir -p $(dirname {output}) && cd tools
        perl -MNet::FTP -e \
            '$ftp = new Net::FTP("ftp.ncbi.nlm.nih.gov", Passive => 1);
             $ftp->login; $ftp->binary;
             $ftp->get("/entrez/entrezdirect/edirect.tar.gz");'
        gunzip -c edirect.tar.gz | tar xf -
        rm edirect.tar.gz
        cd edirect && wget https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/xtract.Linux.gz && gunzip -f xtract.Linux.gz && chmod +x xtract.Linux
        touch $(basename {output})
        """

# Make queries & filtering
##################################################
rule query_plasmids:
    output:
        "{dir}/{date}__{db}__query.txt"
    message:
        "Query for plasmids in {wildcards.db}"
    params:
        query=lambda wildcards: config['eutils']['query']['plasmid'].format(db=wildcards.db, path=config['eutils']['dir']),
        header='\t'.join(config['eutils']['header']['plasmid'])
    shell:
        """
        mkdir -p $(dirname {output}) &&
        echo -e \"{params.header}\" > {output}
        {params.query} >> {output}
        """

rule filter_plasmids:
    input:
        "{dir}/{date}__{db}__query.txt"
    output:
        "{dir}/{date}__{db}__filtered.txt"
    message:
        "Filter plasmids in {input}"
    run:
        df = pandas.read_csv(filepath_or_buffer=input[0], sep='\t', header=0)
        df.fillna(value="NA", axis="columns", inplace=True)
        print(df.head())
        print(df.groupby('Completeness')['UID'].nunique())
        print(df.groupby('Genome')['UID'].nunique())
        print('Min length = {}; max length = {}'.format(min(df['Length']), max(df['Length'])))
        total = df.shape[0]
        df = df.loc[((df['Completeness'] == "NA") | (df['Completeness'] == "complete")) & ((df['Genome'] == "NA") | (df['Genome'] == "plasmid")) & (df['Length'] >= 100),]
        print('Filtering: {} of {} entries were kept'.format(df.shape[0], total))
        df.to_csv(output[0], sep='\t', header=True, index=False, index_label=False)

rule query_biosamples:
    input:
        "{dir}/{date}__{db}__filtered.txt"
    output:
        "{dir}/{date}__{db}__biosamples.txt"
    message:
        "Query for biosamples from {input}"
    params:
        query_before=config['eutils']['query']['biosample']['before'].format(path=config['eutils']['dir']),
        query_after=config['eutils']['query']['biosample']['after'].format(path=config['eutils']['dir']),
        header='\t'.join(config['eutils']['header']['biosample'])
    shell:
        """
        echo -e \"{params.header}\" > {output} &&
        {params.query_before} \"$(tail -n +2 {input} | cut -f8 | grep -v '^NA$' | sort | uniq | tr \'\n\' \' \')\" {params.query_after} >> {output}
        """

rule query_taxon:
    input:
        "{dir}/{date}__{db}__filtered.txt"
    output:
        "{dir}/{date}__{db}__taxa.txt"
    message:
        "Query for taxa from {input}"
    params:
        query_before=config['eutils']['query']['taxon']['before'].format(path=config['eutils']['dir']),
        query_after=config['eutils']['query']['taxon']['after'].format(path=config['eutils']['dir'])
    shell:
        """
        {params.query_before} \"$(tail -n +2 {input} | cut -f10 | grep -v '^NA$' | sort | uniq | tr \'\n\' \' \')\" {params.query_after} > {output}
        """

rule proc_taxon:
    input:
        "{dir}/{date}__{db}__taxa.txt"
    output:
        "{dir}/{date}__{db}__taxaproc.txt"
    message:
        "Process taxa from {input}"
    run:
        header  = ["taxon_id", "taxon_name", "taxon_rank", "lineage"]
        for rank in config['eutils']['header']['ranks']:
            header += ['taxon_%s_id' % rank, 'taxon_%s_name' % rank]
        with open(output[0], 'w') as ofile:
            ofile.write('\t'.join(header) + '\n')
            with open(input[0], 'r') as ifile:
                for line in ifile:
                    info = dict.fromkeys(header,'NA')
                    line = line.rstrip('\n')
                    for field in line.split('\t'):
                        if re.search('\|',field):
                            tid, tname, trank = field.split('|')
                            if trank == 'no rank':
                                trank = 'NA'
                            elif "taxon_%s_name" % trank in header:
                                info["taxon_%s_name" % trank] = tname
                                info["taxon_%s_id" % trank] = tid
                            if info['taxon_id'] == 'NA':
                                info['taxon_id'] = tid
                                info['taxon_name'] = tname
                                info['taxon_rank'] = trank
                        else: # lineage
                            info['lineage'] = field
                    assert info['taxon_id'] != "NA" and info['taxon_name'] != "NA"
                    ofile.write('\t'.join([info[k] for k in header]) + '\n')

# Locations
##################################################
rule parse_locations:
    input:
        tab="{dir}/{date}__{db}__biosamples.txt",
        pck=config['data']['locs']
    output:
        "{dir}/{date}__{db}__biosamples_locs.txt"
    message:
        "Parsing locations from {input}"
    params:
        api_keys=config['data']['gmaps_api_keys']
    run:
        setup_logger()
        # api keys
        with open(params.api_keys, 'r') as ifile:
            api_key = [k.rstrip('\n').strip() for k in ifile.readlines()]
            api_key = api_key.pop()
        logging.info('Using API key: %s' % api_key)
        # load known locations
        locs = dict()
        if os.path.exists(input.pck):
            with open(input.pck, 'rb') as ifile:
                locs = pickle.load(ifile)
        # sample table with locations
        df = pandas.read_csv(input.tab, header=0, index_col=0, sep='\t')
        df['loc_lat'] = None
        df['loc_lng'] = None
        # parse
        for i in df.index:
            # location name and coordinates
            l_n = df.loc[i,'Location']
            l_c = df.loc[i,'Coordinates']

            # pre-processing
            l_n = preproc_loc_str(l_n)
            l_c = preproc_loc_coords(preproc_loc_str(l_c))

            # no location data
            if l_n is None and l_c is None:
                continue

            # at least name or coordinates given
            if l_c is not None:
                if l_c not in locs:
                    logging.info('Retrieving coordinates for location: \"{}\"'.format(l_c))
                    try:
                        locs[l_c] = parse_location(loc_str=l_c, api_key=api_key, is_name=False)
                        with open(input.pck, "wb") as ofile:
                            pickle.dump(locs, ofile)
                    except Exception as e:
                        logging.error('Error while retrieving coordinates for location \"{}\"'.format(l_c))
                        raise(e)
                df.loc[i,'loc_lat'] = locs[l_c]['lat']
                df.loc[i,'loc_lng'] = locs[l_c]['lng']
            elif l_n is not None:
                if l_n not in locs:
                    logging.info('Retrieving coordinates for location: \"{}\"'.format(l_n))
                    try:
                        locs[l_n] = parse_location(loc_str=l_n, api_key=api_key, is_name=True)
                        with open(input.pck, "wb") as ofile:
                            pickle.dump(locs, ofile)
                    except Exception as e:
                        logging.error('Error while retrieving coordinates for location \"{}\"'.format(l_n))
                        raise(e)
                df.loc[i,'loc_lat'] = locs[l_n]['lat']
                df.loc[i,'loc_lng'] = locs[l_n]['lng']
        df.to_csv(output[0], sep='\t', header=True, index=True, index_label='ACC')

# Master files
##################################################
# Download FASTAs
rule download_seqs:
    input:
        "%s/{date}__{db}__filtered.txt" % config['data']['odir']['reports']
    output:
        "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    message:
        "Downloading sequences for IDs in {input}"
    params:
        eutils=config['eutils']['dir']
    shell:
        """
        mkdir -p $(dirname {output}) &&
        python src/download_fastas.py -t {input} -i \"ACC\" -o {output} -c 20 -s 500 -e {params.eutils} 2>&1 | tee {output}.log &&
        cat {output}.tmp.* > {output} && rm {output}.tmp.*
        """

# BLASTn databases
rule blastndb:
    input:
        "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    output:
        ["%s/{date}__{db}.fna.%s" % (config["data"]["odir"]["master"], ext) for ext in blastdb_ext]
    log:
        "%s/{date}__{db}.makeblastdb.log" % config["data"]["odir"]["master"]
    params:
        bin=BIN_MAKEBLASTDB
    message:
        "Create BLASTDB from {input}"
    shell:
        "{params.bin} -in {input} -input_type fasta -dbtype nucl -title \"ncbi__{wildcards.db}__{wildcards.date}\" -logfile {log}"

# Mash
rule mash_sig:
    input:
        "%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.msh" % config["data"]["odir"]["master"]
    log:
        "%s/{date}__{db}.msh.log" % config["data"]["odir"]["master"]
    params:
        params=config['mash']['sketch_params'],
        mash=BIN_MASH
    message:
        "Create signatures using Mash on {input}"
    shell:
        "{params.mash} sketch {params.params} -o $(dirname {output})/$(basename -s .msh {output}) {input} 2>&1 | tee {log}"

rule mash_dist:
    input:
        "%s/{date}__{db}.msh" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.dist" % config["data"]["odir"]["master"]
    log:
        "%s/{date}__{db}.dist.log" % config["data"]["odir"]["master"]
    params:
        params=config['mash']['dist_params'],
        mash=BIN_MASH
    message:
        "Compare signatures in {input}"
    shell:
        "{params.mash} dist {params.params} {input} {input} > {output} 2> {log}"

# UMAP
rule umap:
    input:
        "%s/{date}__{db}.dist" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.umap" % config["data"]["odir"]["master"]
    log:
        "%s/{date}__{db}.umap.log" % config["data"]["odir"]["master"]
    params:
        neighbors=config['umap']['neighbors'],
        components=config['umap']['components']
    message:
        "UMAP on {input}"
    run:
        import umap
        import numpy
        setup_logger()
        # IDs
        ids = None
        with open(input[0], 'r') as ifile:
            ids = ifile.readline().rstrip('\n').split('\t')[1:]
            logging.info('Distance matrix: {num} x {num}'.format(num=len(ids)))
        # distance matrix
        logging.info('Start loading distances...')
        dist = numpy.loadtxt(
            fname=input[0],
            skiprows=1,
            usecols=range(1,len(ids) + 1) # skip 1st (contains IDs)
        )
        logging.info('Done.')
        # embedding
        logging.info('Start embedding...')
        embedding = umap.UMAP(
            n_neighbors=params.neighbors,
            n_components=params.components,
            init='random',
            metric='precomputed'
        ).fit_transform(dist)
        logging.info('Done.')
        # save to file
        embedding = pandas.DataFrame(
            embedding,
            columns=['D1', 'D2'],
            index=ids
        )
        embedding.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=True,
            index_label='ID'
        )

# Table
rule infotable:
    input:
        plasmids="%s/{date}__{db}__filtered.txt" % config["data"]["odir"]["reports"],
        samples="%s/{date}__{db}__biosamples_locs.txt" % config["data"]["odir"]["reports"],
        taxa="%s/{date}__{db}__taxaproc.txt" % config["data"]["odir"]["reports"],
        embedding="%s/{date}__{db}.umap" % config["data"]["odir"]["master"],
        fasta="%s/{date}__{db}.fna" % config["data"]["odir"]["master"]
    output:
        "%s/{date}__{db}.tsv" % config["data"]["odir"]["master"]
    message:
        "Create info table for records in {input.plasmids}"
    run:
        from Bio import SeqIO
        from Bio.SeqUtils import GC
        setup_logger()

        # read in data
        # plasmids
        pls = pandas.read_csv(input.plasmids, sep='\t', header=0, index_col=None)
        pls.set_index(keys='ACC', drop=False, inplace=True, verify_integrity=True)
        pls['ProjectID'].replace(0, None)
        # samples
        smp = pandas.read_csv(input.samples, sep='\t', header=0, index_col=0)
        # taxonomy
        tax = pandas.read_csv(input.taxa, sep='\t', header=0, index_col=None)
        # embedding
        emb = pandas.read_csv(input.embedding, sep='\t', header=0, index_col='ID')

        # add info from FASTA
        logging.info('Adding info from FASTA {}'.format(input.fasta))
        pls['ACCFasta'] = None
        pls['GC'] = None
        with open(input.fasta, 'r') as ifile:
            for record in tqdm(SeqIO.parse(ifile, 'fasta')):
                assert re.fullmatch(r'\w+\.\d+', record.id), 'Unexpected ID format in FASTA: {}'.format(record.id)
                rid = record.id.split('.')[0]
                assert rid in pls.index.values, 'FASTA record ID {}: not in plasmid table'.format(rid)
                assert len(record.seq) == pls.loc[rid,'Length'], 'FASTA record ID {}: {} vs {} as length'.format(rid, len(record.seq), pls.loc[rid,'Length'])
                pls.loc[rid,'ACCFasta'] = record.id
                pls.loc[rid,'GC'] = GC(str(record.seq))

        # add info from other tables
        logging.info('Adding info from {}'.format(input.samples))
        pls = pandas.merge(
            left=pls,
            right=smp,
            how='left',
            left_on='BioSampleACC',
            right_index=True,
            sort=False
        )
        logging.info('Adding info from {}'.format(input.taxa))
        pls = pandas.merge(
            left=pls,
            right=tax,
            how='left',
            left_on='TaxonID',
            right_on='taxon_id',
            sort=False
        )
        pls = pandas.merge(
            left=pls,
            right=emb,
            how='left',
            left_on='ACCFasta',
            right_index=True,
            sort=False
        )

        # save
        pls.to_csv(
            path_or_buf=output[0],
            sep='\t',
            header=True,
            index=False,
            index_label=False
        )
